{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangSmith 向量检索评估教程\n",
        "\n",
        "## 目录\n",
        "1. [LangSmith平台概述](#概述)\n",
        "2. [安装和环境配置](#安装)\n",
        "3. [LangSmith评估器类型](#评估器类型)\n",
        "4. [创建数据集](#创建数据集)\n",
        "5. [向量检索评估器实现](#评估器实现)\n",
        "6. [运行评估](#运行评估)\n",
        "7. [完整示例代码](#完整示例)\n",
        "8. [结果分析和优化建议](#分析优化)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. LangSmith平台概述 {#概述}\n",
        "\n",
        "LangSmith是LangChain官方提供的评估、调试和监控平台，专为LLM应用设计。\n",
        "\n",
        "### 核心特点：\n",
        "- ✅ **可视化界面**：提供直观的评估结果展示和分析\n",
        "- ✅ **多种评估器**：支持LLM-as-Judge、自定义评估器、嵌入距离等\n",
        "- ✅ **数据集管理**：方便管理和版本控制评估数据集\n",
        "- ✅ **实验追踪**：记录每次评估的结果，便于对比和优化\n",
        "- ✅ **生产监控**：可以监控生产环境的RAG系统性能\n",
        "\n",
        "### LangSmith评估流程：\n",
        "```\n",
        "创建数据集 → 定义评估器 → 运行评估\n",
        "              ↓\n",
        "        LangSmith UI查看结果\n",
        "              ↓\n",
        "        分析并优化RAG系统\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 安装和环境配置 {#安装}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装LangSmith和相关依赖\n",
        "%pip install langsmith langchain-openai langchain-community -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 配置环境变量\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 加载.env文件（如果存在）\n",
        "load_dotenv()\n",
        "\n",
        "# 设置LangSmith API密钥（需要从 https://smith.langchain.com/ 获取）\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = \"your-langsmith-api-key\"\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-evaluation\"  # 项目名称\n",
        "\n",
        "# 设置OpenAI API密钥（用于LLM评估器）\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
        "\n",
        "# 导入必要的库\n",
        "from langsmith import Client\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "from typing import Dict, List, Any\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"环境配置完成！\")\n",
        "print(\"\\n注意：请确保已设置以下环境变量：\")\n",
        "print(\"  - LANGSMITH_API_KEY\")\n",
        "print(\"  - OPENAI_API_KEY (如果使用OpenAI作为评估器)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. LangSmith评估器类型 {#评估器类型}\n",
        "\n",
        "LangSmith支持多种类型的评估器：\n",
        "\n",
        "### 3.1 LLM-as-Judge评估器\n",
        "使用大语言模型作为评判者，自动评估答案质量。这是最常用的评估方式。\n",
        "\n",
        "**优点**：\n",
        "- 自动化程度高\n",
        "- 可以评估复杂的主观性问题\n",
        "- 不需要人工标注\n",
        "\n",
        "**缺点**：\n",
        "- 需要调用LLM API，有成本\n",
        "- 评估结果可能有一定随机性\n",
        "\n",
        "### 3.2 自定义评估器\n",
        "根据具体需求编写评估逻辑，可以结合规则、语义相似度等多种方法。\n",
        "\n",
        "**适用场景**：\n",
        "- 有明确的评估规则\n",
        "- 需要评估特定维度\n",
        "- 希望控制评估成本\n",
        "\n",
        "### 3.3 嵌入距离评估器\n",
        "使用嵌入向量计算相似度，评估检索结果的相关性。\n",
        "\n",
        "**适用场景**：\n",
        "- 评估检索质量\n",
        "- 快速评估大量样本\n",
        "- 不需要深度语义理解时\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 创建数据集 {#创建数据集}\n",
        "\n",
        "在LangSmith中，我们需要先创建数据集，然后对数据集进行评估。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 初始化LangSmith客户端\n",
        "# 注意：需要先设置LANGSMITH_API_KEY环境变量\n",
        "try:\n",
        "    client = Client()\n",
        "    print(\"LangSmith客户端初始化成功！\")\n",
        "except Exception as e:\n",
        "    print(f\"初始化失败：{str(e)}\")\n",
        "    print(\"请确保已设置LANGSMITH_API_KEY环境变量\")\n",
        "    print(\"可以在 https://smith.langchain.com/ 获取API密钥\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建评估数据集\n",
        "def create_evaluation_dataset(client, dataset_name: str, examples: List[Dict]):\n",
        "    \"\"\"\n",
        "    创建LangSmith评估数据集\n",
        "    \n",
        "    Args:\n",
        "        client: LangSmith客户端\n",
        "        dataset_name: 数据集名称\n",
        "        examples: 示例列表，每个示例包含inputs和outputs\n",
        "    \n",
        "    Returns:\n",
        "        str: 数据集名称\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 检查数据集是否已存在\n",
        "        try:\n",
        "            dataset = client.read_dataset(dataset_name=dataset_name)\n",
        "            print(f\"数据集 '{dataset_name}' 已存在，将添加新示例\")\n",
        "            dataset_id = dataset.id\n",
        "        except:\n",
        "            # 创建新数据集\n",
        "            dataset = client.create_dataset(\n",
        "                dataset_name=dataset_name,\n",
        "                description=\"RAG系统评估数据集\"\n",
        "            )\n",
        "            dataset_id = dataset.id\n",
        "            print(f\"创建新数据集 '{dataset_name}'\")\n",
        "        \n",
        "        # 添加示例\n",
        "        for example in examples:\n",
        "            client.create_example(\n",
        "                inputs=example[\"inputs\"],\n",
        "                outputs=example.get(\"outputs\", {}),\n",
        "                dataset_id=dataset_id\n",
        "            )\n",
        "        \n",
        "        print(f\"已添加 {len(examples)} 个示例到数据集\")\n",
        "        return dataset_name\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"创建数据集时出错：{str(e)}\")\n",
        "        return None\n",
        "\n",
        "# 准备示例数据\n",
        "evaluation_examples = [\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"RAG是什么？\",\n",
        "            \"documents\": [\n",
        "                \"RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。\",\n",
        "                \"RAG的核心思想是在生成答案前，先从知识库中检索相关文档。\",\n",
        "                \"RAG可以解决大模型的知识盲区问题，让模型能够访问非公开知识。\"\n",
        "            ]\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"answer\": \"RAG是一种结合检索和生成的技术，它通过在生成答案前从知识库检索相关文档来解决大模型的知识盲区问题。\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"RAG的核心思想是什么？\",\n",
        "            \"documents\": [\n",
        "                \"RAG的核心思想是在生成答案前，先从知识库中检索相关文档。\",\n",
        "                \"检索到的文档作为上下文输入给生成模型，帮助生成更准确的答案。\"\n",
        "            ]\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"answer\": \"RAG的核心思想是在生成答案前先从知识库检索相关文档，然后将检索到的文档作为上下文输入给生成模型。\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"RAG能解决什么问题？\",\n",
        "            \"documents\": [\n",
        "                \"RAG可以解决大模型的知识盲区问题，让模型能够访问非公开知识。\",\n",
        "                \"通过RAG，模型可以动态检索最新的信息，而不需要重新训练。\"\n",
        "            ]\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"answer\": \"RAG可以解决大模型的知识盲区问题，让模型能够访问非公开知识，并且可以动态检索最新信息而不需要重新训练。\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# 创建数据集（如果已初始化客户端）\n",
        "if 'client' in locals():\n",
        "    dataset_name = create_evaluation_dataset(\n",
        "        client=client,\n",
        "        dataset_name=\"rag-evaluation-dataset\",\n",
        "        examples=evaluation_examples\n",
        "    )\n",
        "else:\n",
        "    print(\"跳过数据集创建（客户端未初始化）\")\n",
        "    dataset_name = \"rag-evaluation-dataset\"  # 示例名称\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 向量检索评估器实现 {#评估器实现}\n",
        "\n",
        "本节实现多个专门用于评估向量检索的评估器。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定义评估结果的类型\n",
        "class CorrectnessGrade(TypedDict):\n",
        "    \"\"\"答案正确性评估结果\"\"\"\n",
        "    explanation: Annotated[str, \"评分理由\"]\n",
        "    correct: Annotated[bool, \"答案是否正确\"]\n",
        "\n",
        "class GroundednessGrade(TypedDict):\n",
        "    \"\"\"答案基于文档的评估结果\"\"\"\n",
        "    explanation: Annotated[str, \"评分理由\"]\n",
        "    grounded: Annotated[bool, \"答案是否基于文档\"]\n",
        "\n",
        "class RelevanceGrade(TypedDict):\n",
        "    \"\"\"答案相关性评估结果\"\"\"\n",
        "    explanation: Annotated[str, \"评分理由\"]\n",
        "    relevant: Annotated[bool, \"答案是否相关\"]\n",
        "\n",
        "class RetrievalPrecisionGrade(TypedDict):\n",
        "    \"\"\"检索精确度评估结果\"\"\"\n",
        "    explanation: Annotated[str, \"评分理由\"]\n",
        "    precision_score: Annotated[float, \"精确度分数 (0-1)\"]\n",
        "\n",
        "class RetrievalRecallGrade(TypedDict):\n",
        "    \"\"\"检索召回率评估结果\"\"\"\n",
        "    explanation: Annotated[str, \"评分理由\"]\n",
        "    recall_score: Annotated[float, \"召回率分数 (0-1)\"]\n",
        "\n",
        "# 初始化LLM评估器（使用结构化输出）\n",
        "try:\n",
        "    grader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\n",
        "        CorrectnessGrade, method=\"json_schema\", strict=True\n",
        "    )\n",
        "    print(\"LLM评估器初始化成功！\")\n",
        "except Exception as e:\n",
        "    print(f\"LLM评估器初始化失败：{str(e)}\")\n",
        "    print(\"请确保已设置OPENAI_API_KEY环境变量\")\n",
        "    grader_llm = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Correctness Evaluator（答案正确性评估器）\n",
        "def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
        "    \"\"\"\n",
        "    对比答案的事实准确性\n",
        "    \n",
        "    Args:\n",
        "        inputs: 包含question和documents的字典\n",
        "        outputs: 包含answer的字典\n",
        "        reference_outputs: 包含标准答案的字典\n",
        "    \n",
        "    Returns:\n",
        "        dict: 包含correctness评分的字典\n",
        "    \"\"\"\n",
        "    if grader_llm is None:\n",
        "        return {\"correctness\": False, \"reason\": \"LLM评估器未初始化\"}\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    问题：{inputs['question']}\n",
        "    参考答案：{reference_outputs.get('answer', '')}\n",
        "    生成答案：{outputs['answer']}\n",
        "    \n",
        "    请评估生成答案的事实准确性。生成答案是否与参考答案在事实层面一致？\n",
        "    \n",
        "    注意：\n",
        "    - 如果生成答案和参考答案在核心事实上一致，即使表达方式不同，也应判定为正确\n",
        "    - 如果生成答案包含错误信息或遗漏关键信息，应判定为不正确\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        grade = grader_llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "        return {\n",
        "            \"correctness\": grade.get(\"correct\", False),\n",
        "            \"explanation\": grade.get(\"explanation\", \"\")\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"correctness\": False, \"reason\": f\"评估失败: {str(e)}\"}\n",
        "\n",
        "print(\"Correctness Evaluator已定义\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Groundedness Evaluator（答案基于文档的评估器）\n",
        "def groundedness_evaluator(inputs: dict, outputs: dict) -> dict:\n",
        "    \"\"\"\n",
        "    检测答案是否基于检索文档，没有幻觉\n",
        "    \n",
        "    Args:\n",
        "        inputs: 包含question和documents的字典\n",
        "        outputs: 包含answer的字典\n",
        "    \n",
        "    Returns:\n",
        "        dict: 包含groundedness评分的字典\n",
        "    \"\"\"\n",
        "    if grader_llm is None:\n",
        "        return {\"groundedness\": False, \"reason\": \"LLM评估器未初始化\"}\n",
        "    \n",
        "    # 将文档列表转换为字符串\n",
        "    doc_string = \"\\n\\n\".join([\n",
        "        f\"文档 {i+1}: {doc}\" \n",
        "        for i, doc in enumerate(inputs.get('documents', []))\n",
        "    ])\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    文档：\n",
        "    {doc_string}\n",
        "    \n",
        "    答案：{outputs['answer']}\n",
        "    \n",
        "    请评估答案是否完全基于上述文档，没有引入文档中没有的信息（幻觉）？\n",
        "    \n",
        "    注意：\n",
        "    - 如果答案中的所有信息都可以从文档中推断出来，判定为基于文档\n",
        "    - 如果答案包含文档中没有的信息，判定为包含幻觉\n",
        "    - 合理的推理和总结是允许的，但不能添加新的事实\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        # 使用临时的结构化输出\n",
        "        groundedness_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\n",
        "            GroundednessGrade, method=\"json_schema\", strict=True\n",
        "        )\n",
        "        grade = groundedness_llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "        return {\n",
        "            \"groundedness\": grade.get(\"grounded\", False),\n",
        "            \"explanation\": grade.get(\"explanation\", \"\")\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"groundedness\": False, \"reason\": f\"评估失败: {str(e)}\"}\n",
        "\n",
        "print(\"Groundedness Evaluator已定义\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Relevance Evaluator（答案相关性评估器）\n",
        "def relevance_evaluator(inputs: dict, outputs: dict) -> dict:\n",
        "    \"\"\"\n",
        "    检测答案与问题的相关性\n",
        "    \n",
        "    Args:\n",
        "        inputs: 包含question的字典\n",
        "        outputs: 包含answer的字典\n",
        "    \n",
        "    Returns:\n",
        "        dict: 包含relevance评分的字典\n",
        "    \"\"\"\n",
        "    if grader_llm is None:\n",
        "        return {\"relevance\": False, \"reason\": \"LLM评估器未初始化\"}\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    问题：{inputs['question']}\n",
        "    答案：{outputs['answer']}\n",
        "    \n",
        "    请评估答案是否有效回答了问题？\n",
        "    \n",
        "    注意：\n",
        "    - 如果答案直接回答了问题，即使不够完整，也应判定为相关\n",
        "    - 如果答案答非所问或偏离主题，应判定为不相关\n",
        "    - 考虑答案的完整性和相关性\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        relevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\n",
        "            RelevanceGrade, method=\"json_schema\", strict=True\n",
        "        )\n",
        "        grade = relevance_llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "        return {\n",
        "            \"relevance\": grade.get(\"relevant\", False),\n",
        "            \"explanation\": grade.get(\"explanation\", \"\")\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"relevance\": False, \"reason\": f\"评估失败: {str(e)}\"}\n",
        "\n",
        "print(\"Relevance Evaluator已定义\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Retrieval Precision Evaluator（检索精确度评估器）\n",
        "def retrieval_precision_evaluator(inputs: dict, outputs: dict) -> dict:\n",
        "    \"\"\"\n",
        "    评估检索到的文档的相关性（精确度）\n",
        "    \n",
        "    Args:\n",
        "        inputs: 包含question和documents的字典\n",
        "        outputs: 包含answer的字典（用于判断文档相关性）\n",
        "    \n",
        "    Returns:\n",
        "        dict: 包含precision_score的字典\n",
        "    \"\"\"\n",
        "    if grader_llm is None:\n",
        "        return {\"precision_score\": 0.0, \"reason\": \"LLM评估器未初始化\"}\n",
        "    \n",
        "    documents = inputs.get('documents', [])\n",
        "    if not documents:\n",
        "        return {\"precision_score\": 0.0, \"reason\": \"没有检索到文档\"}\n",
        "    \n",
        "    # 评估每个文档的相关性\n",
        "    relevant_count = 0\n",
        "    total_count = len(documents)\n",
        "    \n",
        "    doc_string = \"\\n\\n\".join([\n",
        "        f\"文档 {i+1}: {doc}\" \n",
        "        for i, doc in enumerate(documents)\n",
        "    ])\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    问题：{inputs['question']}\n",
        "    答案：{outputs['answer']}\n",
        "    \n",
        "    检索到的文档：\n",
        "    {doc_string}\n",
        "    \n",
        "    请评估每个文档对回答这个问题的相关性。对于每个文档，判断它是否有助于回答这个问题。\n",
        "    \n",
        "    请返回一个分数，表示相关文档的比例（0-1之间）。\n",
        "    例如，如果5个文档中有3个相关，则返回0.6。\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        precision_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\n",
        "            RetrievalPrecisionGrade, method=\"json_schema\", strict=True\n",
        "        )\n",
        "        grade = precision_llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "        return {\n",
        "            \"precision_score\": grade.get(\"precision_score\", 0.0),\n",
        "            \"explanation\": grade.get(\"explanation\", \"\")\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # 如果评估失败，使用简单的启发式方法\n",
        "        return {\"precision_score\": 0.5, \"reason\": f\"评估失败: {str(e)}\"}\n",
        "\n",
        "print(\"Retrieval Precision Evaluator已定义\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Retrieval Recall Evaluator（检索召回率评估器）\n",
        "def retrieval_recall_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
        "    \"\"\"\n",
        "    评估检索到的文档是否覆盖了所有相关信息（召回率）\n",
        "    \n",
        "    Args:\n",
        "        inputs: 包含question和documents的字典\n",
        "        outputs: 包含answer的字典\n",
        "        reference_outputs: 包含ground_truth_documents的字典\n",
        "    \n",
        "    Returns:\n",
        "        dict: 包含recall_score的字典\n",
        "    \"\"\"\n",
        "    if grader_llm is None:\n",
        "        return {\"recall_score\": 0.0, \"reason\": \"LLM评估器未初始化\"}\n",
        "    \n",
        "    retrieved_docs = inputs.get('documents', [])\n",
        "    ground_truth_docs = reference_outputs.get('ground_truth_documents', [])\n",
        "    \n",
        "    if not ground_truth_docs:\n",
        "        return {\"recall_score\": 0.0, \"reason\": \"没有提供ground truth文档\"}\n",
        "    \n",
        "    retrieved_string = \"\\n\\n\".join([\n",
        "        f\"检索文档 {i+1}: {doc}\" \n",
        "        for i, doc in enumerate(retrieved_docs)\n",
        "    ])\n",
        "    \n",
        "    ground_truth_string = \"\\n\\n\".join([\n",
        "        f\"参考文档 {i+1}: {doc}\" \n",
        "        for i, doc in enumerate(ground_truth_docs)\n",
        "    ])\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    问题：{inputs['question']}\n",
        "    \n",
        "    检索到的文档：\n",
        "    {retrieved_string}\n",
        "    \n",
        "    应该检索到的参考文档（ground truth）：\n",
        "    {ground_truth_string}\n",
        "    \n",
        "    请评估检索到的文档是否覆盖了所有应该检索到的参考文档中的信息。\n",
        "    \n",
        "    返回一个分数（0-1之间），表示检索到的文档覆盖了多少参考文档中的信息。\n",
        "    例如，如果参考文档中有3个关键信息点，检索到的文档覆盖了2个，则返回0.67。\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        recall_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\n",
        "            RetrievalRecallGrade, method=\"json_schema\", strict=True\n",
        "        )\n",
        "        grade = recall_llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "        return {\n",
        "            \"recall_score\": grade.get(\"recall_score\", 0.0),\n",
        "            \"explanation\": grade.get(\"explanation\", \"\")\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"recall_score\": 0.0, \"reason\": f\"评估失败: {str(e)}\"}\n",
        "\n",
        "print(\"Retrieval Recall Evaluator已定义\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 运行评估 {#运行评估}\n",
        "\n",
        "在LangSmith中，我们可以使用`client.evaluate()`方法来运行评估。该方法会：\n",
        "1. 对数据集中的每个示例运行RAG系统\n",
        "2. 使用定义的评估器评估结果\n",
        "3. 在LangSmith UI中展示评估结果\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建一个简单的RAG系统函数用于评估\n",
        "def simple_rag_system(question: str, documents: list = None) -> dict:\n",
        "    \"\"\"\n",
        "    简单的RAG系统示例\n",
        "    在实际使用中，应该替换为你的实际RAG系统\n",
        "    \n",
        "    Args:\n",
        "        question: 用户问题\n",
        "        documents: 检索到的文档列表\n",
        "    \n",
        "    Returns:\n",
        "        dict: 包含answer的字典\n",
        "    \"\"\"\n",
        "    # 如果没有提供文档，使用默认文档\n",
        "    if documents is None:\n",
        "        documents = [\n",
        "            \"RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。\",\n",
        "            \"RAG的核心思想是在生成答案前，先从知识库中检索相关文档。\",\n",
        "            \"RAG可以解决大模型的知识盲区问题，让模型能够访问非公开知识。\"\n",
        "        ]\n",
        "    \n",
        "    # 这里应该是实际的LLM调用\n",
        "    # 为了演示，返回一个示例答案\n",
        "    answer = \"RAG是一种结合检索和生成的技术，它通过在生成答案前从知识库检索相关文档来解决大模型的知识盲区问题。\"\n",
        "    \n",
        "    return {\"answer\": answer}\n",
        "\n",
        "print(\"RAG系统函数已创建\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备评估器列表\n",
        "evaluators = [\n",
        "    correctness_evaluator,\n",
        "    groundedness_evaluator,\n",
        "    relevance_evaluator,\n",
        "    retrieval_precision_evaluator,\n",
        "    # retrieval_recall_evaluator,  # 需要reference_outputs，可能需要特殊处理\n",
        "]\n",
        "\n",
        "print(\"已准备评估器列表\")\n",
        "print(f\"评估器数量：{len(evaluators)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 运行评估（如果客户端已初始化）\n",
        "if 'client' in locals() and dataset_name:\n",
        "    try:\n",
        "        print(f\"开始评估数据集：{dataset_name}\")\n",
        "        print(\"注意：这将调用LLM API，可能需要一些时间...\")\n",
        "        \n",
        "        # 运行评估\n",
        "        experiment_results = client.evaluate(\n",
        "            target=simple_rag_system,  # 目标函数\n",
        "            data=dataset_name,  # 数据集名称\n",
        "            evaluators=evaluators,  # 评估器列表\n",
        "            experiment_prefix=\"rag-evaluation\",  # 实验前缀\n",
        "            max_concurrency=2,  # 最大并发数\n",
        "        )\n",
        "        \n",
        "        print(\"\\n评估完成！\")\n",
        "        print(f\"评估结果数量：{len(list(experiment_results))}\")\n",
        "        print(\"\\n你可以在LangSmith UI中查看详细结果：\")\n",
        "        print(\"https://smith.langchain.com/\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"运行评估时出错：{str(e)}\")\n",
        "        print(\"\\n可能的原因：\")\n",
        "        print(\"1. 未设置LANGSMITH_API_KEY环境变量\")\n",
        "        print(\"2. 数据集不存在或格式不正确\")\n",
        "        print(\"3. 评估器函数签名不匹配\")\n",
        "        print(\"4. 网络连接问题\")\n",
        "else:\n",
        "    print(\"跳过评估（客户端未初始化或数据集不存在）\")\n",
        "    print(\"\\n示例代码：\")\n",
        "    print(\"\"\"\n",
        "    # 运行评估的代码示例：\n",
        "    experiment_results = client.evaluate(\n",
        "        target=your_rag_system_function,\n",
        "        data=\"your-dataset-name\",\n",
        "        evaluators=[correctness_evaluator, groundedness_evaluator, ...],\n",
        "        experiment_prefix=\"rag-evaluation\",\n",
        "        max_concurrency=2,\n",
        "    )\n",
        "    \"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 本地评估示例（不依赖LangSmith服务器）\n",
        "# 这个方法可以在没有LangSmith API的情况下进行本地评估\n",
        "\n",
        "def local_evaluate(rag_system, test_dataset, evaluators):\n",
        "    \"\"\"\n",
        "    本地评估函数，不依赖LangSmith服务器\n",
        "    \n",
        "    Args:\n",
        "        rag_system: RAG系统函数\n",
        "        test_dataset: 测试数据集\n",
        "        evaluators: 评估器列表\n",
        "    \n",
        "    Returns:\n",
        "        list: 评估结果列表\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for idx, test_case in enumerate(test_dataset):\n",
        "        print(f\"\\n处理测试用例 {idx + 1}/{len(test_dataset)}: {test_case['inputs']['question']}\")\n",
        "        \n",
        "        # 运行RAG系统\n",
        "        rag_output = rag_system(\n",
        "            question=test_case['inputs']['question'],\n",
        "            documents=test_case['inputs'].get('documents', [])\n",
        "        )\n",
        "        \n",
        "        # 准备评估输入\n",
        "        inputs = test_case['inputs']\n",
        "        outputs = rag_output\n",
        "        reference_outputs = test_case.get('outputs', {})\n",
        "        \n",
        "        # 运行每个评估器\n",
        "        case_result = {\n",
        "            'question': inputs['question'],\n",
        "            'answer': outputs.get('answer', ''),\n",
        "            'evaluations': {}\n",
        "        }\n",
        "        \n",
        "        for evaluator in evaluators:\n",
        "            try:\n",
        "                # 根据评估器签名决定如何调用\n",
        "                if 'reference_outputs' in evaluator.__code__.co_varnames:\n",
        "                    eval_result = evaluator(inputs, outputs, reference_outputs)\n",
        "                else:\n",
        "                    eval_result = evaluator(inputs, outputs)\n",
        "                \n",
        "                # 提取评估器名称\n",
        "                eval_name = evaluator.__name__.replace('_evaluator', '')\n",
        "                case_result['evaluations'][eval_name] = eval_result\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  评估器 {evaluator.__name__} 失败: {str(e)}\")\n",
        "        \n",
        "        results.append(case_result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 如果有测试数据，可以进行本地评估\n",
        "if 'evaluation_examples' in locals():\n",
        "    print(\"开始本地评估...\")\n",
        "    local_results = local_evaluate(\n",
        "        rag_system=simple_rag_system,\n",
        "        test_dataset=evaluation_examples,\n",
        "        evaluators=evaluators[:3]  # 只使用前3个评估器\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"本地评估结果：\")\n",
        "    print(\"=\" * 60)\n",
        "    for result in local_results:\n",
        "        print(f\"\\n问题：{result['question']}\")\n",
        "        print(f\"答案：{result['answer'][:100]}...\")\n",
        "        print(\"评估结果：\")\n",
        "        for eval_name, eval_result in result['evaluations'].items():\n",
        "            print(f\"  {eval_name}: {eval_result}\")\n",
        "else:\n",
        "    print(\"本地评估示例代码已准备就绪\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 完整示例代码 {#完整示例}\n",
        "\n",
        "以下是一个完整的LangSmith RAG评估示例，展示了如何评估一个完整的RAG系统。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 完整的LangSmith RAG评估器类\n",
        "class LangSmithRAGEvaluator:\n",
        "    \"\"\"完整的LangSmith RAG评估器\"\"\"\n",
        "    \n",
        "    def __init__(self, rag_system, client=None):\n",
        "        \"\"\"\n",
        "        初始化评估器\n",
        "        \n",
        "        Args:\n",
        "            rag_system: RAG系统函数或对象\n",
        "            client: LangSmith客户端（可选）\n",
        "        \"\"\"\n",
        "        self.rag_system = rag_system\n",
        "        self.client = client\n",
        "        \n",
        "        # 定义评估器\n",
        "        self.evaluators = {\n",
        "            \"correctness\": correctness_evaluator,\n",
        "            \"groundedness\": groundedness_evaluator,\n",
        "            \"relevance\": relevance_evaluator,\n",
        "            \"retrieval_precision\": retrieval_precision_evaluator,\n",
        "            \"retrieval_recall\": retrieval_recall_evaluator,\n",
        "        }\n",
        "    \n",
        "    def create_dataset(self, dataset_name: str, examples: List[Dict]):\n",
        "        \"\"\"\n",
        "        创建LangSmith数据集\n",
        "        \n",
        "        Args:\n",
        "            dataset_name: 数据集名称\n",
        "            examples: 示例列表\n",
        "        \n",
        "        Returns:\n",
        "            str: 数据集名称\n",
        "        \"\"\"\n",
        "        if self.client is None:\n",
        "            print(\"警告：LangSmith客户端未初始化，无法创建数据集\")\n",
        "            return None\n",
        "        \n",
        "        return create_evaluation_dataset(self.client, dataset_name, examples)\n",
        "    \n",
        "    def evaluate_on_langsmith(self, dataset_name: str, experiment_prefix: str = \"rag-eval\"):\n",
        "        \"\"\"\n",
        "        在LangSmith平台上运行评估\n",
        "        \n",
        "        Args:\n",
        "            dataset_name: 数据集名称\n",
        "            experiment_prefix: 实验前缀\n",
        "        \n",
        "        Returns:\n",
        "            Generator: 评估结果生成器\n",
        "        \"\"\"\n",
        "        if self.client is None:\n",
        "            raise ValueError(\"LangSmith客户端未初始化\")\n",
        "        \n",
        "        return self.client.evaluate(\n",
        "            target=self.rag_system,\n",
        "            data=dataset_name,\n",
        "            evaluators=list(self.evaluators.values()),\n",
        "            experiment_prefix=experiment_prefix,\n",
        "            max_concurrency=2,\n",
        "        )\n",
        "    \n",
        "    def evaluate_locally(self, test_dataset: List[Dict]):\n",
        "        \"\"\"\n",
        "        本地评估（不依赖LangSmith服务器）\n",
        "        \n",
        "        Args:\n",
        "            test_dataset: 测试数据集\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame: 评估结果DataFrame\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        \n",
        "        for test_case in test_dataset:\n",
        "            # 运行RAG系统\n",
        "            inputs = test_case['inputs']\n",
        "            rag_output = self.rag_system(\n",
        "                question=inputs['question'],\n",
        "                documents=inputs.get('documents', [])\n",
        "            )\n",
        "            \n",
        "            outputs = rag_output if isinstance(rag_output, dict) else {\"answer\": str(rag_output)}\n",
        "            reference_outputs = test_case.get('outputs', {})\n",
        "            \n",
        "            # 运行评估器\n",
        "            case_result = {\n",
        "                'question': inputs['question'],\n",
        "                'answer': outputs.get('answer', ''),\n",
        "            }\n",
        "            \n",
        "            for eval_name, evaluator in self.evaluators.items():\n",
        "                try:\n",
        "                    # 根据评估器签名决定如何调用\n",
        "                    if 'reference_outputs' in evaluator.__code__.co_varnames:\n",
        "                        eval_result = evaluator(inputs, outputs, reference_outputs)\n",
        "                    else:\n",
        "                        eval_result = evaluator(inputs, outputs)\n",
        "                    \n",
        "                    # 提取分数\n",
        "                    if isinstance(eval_result, dict):\n",
        "                        # 尝试提取分数\n",
        "                        score_key = eval_name.replace('_', '_') + '_score'\n",
        "                        if score_key in eval_result:\n",
        "                            case_result[eval_name] = eval_result[score_key]\n",
        "                        elif eval_name in eval_result:\n",
        "                            case_result[eval_name] = eval_result[eval_name]\n",
        "                        else:\n",
        "                            # 尝试提取布尔值\n",
        "                            for key in ['correct', 'grounded', 'relevant']:\n",
        "                                if key in eval_result:\n",
        "                                    case_result[eval_name] = eval_result[key]\n",
        "                                    break\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"评估器 {eval_name} 失败: {str(e)}\")\n",
        "                    case_result[eval_name] = None\n",
        "            \n",
        "            results.append(case_result)\n",
        "        \n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "# 使用示例\n",
        "print(\"LangSmith RAG评估器已创建\")\n",
        "print(\"\\n使用方法：\")\n",
        "print(\"1. 初始化：evaluator = LangSmithRAGEvaluator(your_rag_system, client)\")\n",
        "print(\"2. 创建数据集：evaluator.create_dataset('my-dataset', examples)\")\n",
        "print(\"3. LangSmith评估：results = evaluator.evaluate_on_langsmith('my-dataset')\")\n",
        "print(\"4. 本地评估：results_df = evaluator.evaluate_locally(test_dataset)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例：使用完整的评估器\n",
        "if 'evaluation_examples' in locals():\n",
        "    # 创建评估器实例\n",
        "    evaluator = LangSmithRAGEvaluator(\n",
        "        rag_system=simple_rag_system,\n",
        "        client=client if 'client' in locals() else None\n",
        "    )\n",
        "    \n",
        "    # 本地评估\n",
        "    print(\"执行本地评估...\")\n",
        "    results_df = evaluator.evaluate_locally(evaluation_examples)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"评估结果：\")\n",
        "    print(\"=\" * 80)\n",
        "    print(results_df.to_string(index=False))\n",
        "    \n",
        "    # 计算平均分数\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"平均评估分数：\")\n",
        "    print(\"=\" * 80)\n",
        "    numeric_columns = results_df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_columns:\n",
        "        if col not in ['question']:\n",
        "            avg_score = results_df[col].mean()\n",
        "            print(f\"{col:25s}: {avg_score:.4f}\")\n",
        "else:\n",
        "    print(\"完整评估器已准备就绪，等待测试数据\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 结果分析和优化建议 {#分析优化}\n",
        "\n",
        "### 8.1 评估结果解读\n",
        "\n",
        "根据LangSmith评估结果，我们可以从以下几个方面进行分析：\n",
        "\n",
        "1. **Correctness < 0.7**: 答案正确性不足\n",
        "   - 优化建议：改进prompt，增加事实检查步骤；使用更强的LLM模型\n",
        "\n",
        "2. **Groundedness < 0.7**: 答案包含幻觉或未基于文档\n",
        "   - 优化建议：改进prompt，明确要求基于文档回答；增加文档验证步骤\n",
        "\n",
        "3. **Relevance < 0.7**: 答案与问题相关性不足\n",
        "   - 优化建议：改进问题理解；优化prompt中的回答要求\n",
        "\n",
        "4. **Retrieval Precision < 0.7**: 检索到的文档相关性不高\n",
        "   - 优化建议：改进embedding模型；增加rerank步骤；优化检索参数\n",
        "\n",
        "5. **Retrieval Recall < 0.7**: 遗漏了相关文档\n",
        "   - 优化建议：增加检索数量k；使用混合检索；改进检索策略\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 评估结果分析函数\n",
        "def analyze_langsmith_results(results_df):\n",
        "    \"\"\"\n",
        "    分析LangSmith评估结果并给出优化建议\n",
        "    \n",
        "    Args:\n",
        "        results_df: 包含评估结果的DataFrame\n",
        "    \n",
        "    Returns:\n",
        "        dict: 分析结果和建议\n",
        "    \"\"\"\n",
        "    analysis = {\n",
        "        \"summary\": {},\n",
        "        \"recommendations\": []\n",
        "    }\n",
        "    \n",
        "    thresholds = {\n",
        "        \"correctness\": 0.7,\n",
        "        \"groundedness\": 0.7,\n",
        "        \"relevance\": 0.7,\n",
        "        \"retrieval_precision\": 0.7,\n",
        "        \"retrieval_recall\": 0.7\n",
        "    }\n",
        "    \n",
        "    for metric, threshold in thresholds.items():\n",
        "        if metric in results_df.columns:\n",
        "            # 处理布尔值列（转换为0/1）\n",
        "            scores = results_df[metric]\n",
        "            if scores.dtype == bool:\n",
        "                scores = scores.astype(int)\n",
        "            \n",
        "            mean_score = scores.mean()\n",
        "            analysis[\"summary\"][metric] = {\n",
        "                \"mean\": mean_score,\n",
        "                \"threshold\": threshold,\n",
        "                \"status\": \"良好\" if mean_score >= threshold else \"需要改进\"\n",
        "            }\n",
        "            \n",
        "            if mean_score < threshold:\n",
        "                if metric == \"correctness\":\n",
        "                    analysis[\"recommendations\"].append(\n",
        "                        \"Correctness得分较低：改进prompt，增加事实检查步骤，使用更强的LLM模型\"\n",
        "                    )\n",
        "                elif metric == \"groundedness\":\n",
        "                    analysis[\"recommendations\"].append(\n",
        "                        \"Groundedness得分较低：改进prompt，明确要求基于文档回答，避免幻觉\"\n",
        "                    )\n",
        "                elif metric == \"relevance\":\n",
        "                    analysis[\"recommendations\"].append(\n",
        "                        \"Relevance得分较低：改进问题理解，优化prompt中的回答要求\"\n",
        "                    )\n",
        "                elif metric == \"retrieval_precision\":\n",
        "                    analysis[\"recommendations\"].append(\n",
        "                        \"Retrieval Precision得分较低：改进embedding模型，增加rerank步骤\"\n",
        "                    )\n",
        "                elif metric == \"retrieval_recall\":\n",
        "                    analysis[\"recommendations\"].append(\n",
        "                        \"Retrieval Recall得分较低：增加检索数量k，使用混合检索策略\"\n",
        "                    )\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "# 如果有评估结果，进行分析\n",
        "if 'results_df' in locals() and not results_df.empty:\n",
        "    analysis = analyze_langsmith_results(results_df)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"评估结果分析：\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"\\n指标概览：\")\n",
        "    for metric, info in analysis[\"summary\"].items():\n",
        "        status_icon = \"✅\" if info[\"status\"] == \"良好\" else \"⚠️\"\n",
        "        print(f\"{status_icon} {metric:25s}: {info['mean']:.4f} (阈值: {info['threshold']:.2f}) - {info['status']}\")\n",
        "    \n",
        "    if analysis[\"recommendations\"]:\n",
        "        print(\"\\n优化建议：\")\n",
        "        for i, rec in enumerate(analysis[\"recommendations\"], 1):\n",
        "            print(f\"{i}. {rec}\")\n",
        "    else:\n",
        "        print(\"\\n✅ 所有指标都达到阈值，系统表现良好！\")\n",
        "else:\n",
        "    print(\"评估结果分析函数已准备就绪\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化评估结果\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # 支持中文\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 如果有评估结果，进行可视化\n",
        "if 'results_df' in locals() and not results_df.empty:\n",
        "    # 提取数值列\n",
        "    numeric_cols = results_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    # 排除非评估指标列\n",
        "    eval_cols = [col for col in numeric_cols if col not in ['question']]\n",
        "    \n",
        "    if eval_cols:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # 1. 各指标平均分数柱状图\n",
        "        metric_means = [results_df[col].mean() for col in eval_cols]\n",
        "        metric_names = [col.replace('_', ' ').title() for col in eval_cols]\n",
        "        \n",
        "        axes[0].bar(metric_names, metric_means, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "        axes[0].set_title('LangSmith评估指标平均分数', fontsize=14, fontweight='bold')\n",
        "        axes[0].set_ylabel('分数', fontsize=12)\n",
        "        axes[0].set_ylim([0, 1])\n",
        "        axes[0].grid(axis='y', alpha=0.3)\n",
        "        axes[0].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # 添加数值标签\n",
        "        for i, v in enumerate(metric_means):\n",
        "            axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "        \n",
        "        # 2. 每个测试用例的分数趋势\n",
        "        x = range(len(results_df))\n",
        "        for col in eval_cols:\n",
        "            scores = results_df[col]\n",
        "            if scores.dtype == bool:\n",
        "                scores = scores.astype(int)\n",
        "            axes[1].plot(x, scores, marker='o', label=col.replace('_', ' ').title(), linewidth=2)\n",
        "        \n",
        "        axes[1].set_title('各测试用例评估分数趋势', fontsize=14, fontweight='bold')\n",
        "        axes[1].set_xlabel('测试用例编号', fontsize=12)\n",
        "        axes[1].set_ylabel('分数', fontsize=12)\n",
        "        axes[1].set_xticks(x)\n",
        "        axes[1].set_xticklabels([f'Case {i+1}' for i in x])\n",
        "        axes[1].legend(loc='best')\n",
        "        axes[1].grid(alpha=0.3)\n",
        "        axes[1].set_ylim([0, 1])\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"没有可可视化的数值评估指标\")\n",
        "else:\n",
        "    print(\"可视化代码已准备就绪，等待评估结果\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 LangSmith UI使用指南\n",
        "\n",
        "LangSmith提供了强大的Web界面来查看和分析评估结果：\n",
        "\n",
        "1. **访问LangSmith UI**：\n",
        "   - 网址：https://smith.langchain.com/\n",
        "   - 使用你的API密钥登录\n",
        "\n",
        "2. **查看评估结果**：\n",
        "   - 在\"Experiments\"页面查看所有评估实验\n",
        "   - 点击实验名称查看详细信息\n",
        "   - 可以查看每个测试用例的评估分数\n",
        "\n",
        "3. **分析功能**：\n",
        "   - 对比不同实验的结果\n",
        "   - 查看评估分数的分布\n",
        "   - 导出评估报告\n",
        "\n",
        "4. **持续监控**：\n",
        "   - 设置定期评估任务\n",
        "   - 监控生产环境的RAG系统性能\n",
        "   - 设置告警阈值\n",
        "\n",
        "### 8.3 最佳实践总结\n",
        "\n",
        "1. **评估数据集构建**：\n",
        "   - 包含不同类型的问题（简单事实、复杂推理、多跳问题）\n",
        "   - 确保有足够的ground truth用于评估\n",
        "   - 定期更新评估数据集\n",
        "\n",
        "2. **评估频率**：\n",
        "   - 每次模型或prompt更新后都应重新评估\n",
        "   - 建立定期评估机制（如每周或每月）\n",
        "   - 记录评估历史，追踪改进效果\n",
        "\n",
        "3. **评估器选择**：\n",
        "   - 根据应用场景选择适当的评估器\n",
        "   - 检索系统重点关注Retrieval Precision和Recall\n",
        "   - 生成系统重点关注Correctness、Groundedness和Relevance\n",
        "\n",
        "4. **优化迭代**：\n",
        "   - 根据评估结果定位问题\n",
        "   - 逐步优化（先优化检索，再优化生成）\n",
        "   - 使用A/B测试验证优化效果\n",
        "\n",
        "### 8.4 RAGAS vs LangSmith对比\n",
        "\n",
        "| 特性 | RAGAS | LangSmith |\n",
        "|------|-------|-----------|\n",
        "| **评估方式** | 无参考评估为主 | 支持有参考和无参考 |\n",
        "| **平台** | 本地/云端 | 云端平台 |\n",
        "| **可视化** | 基础可视化 | 强大的Web UI |\n",
        "| **数据集管理** | 需要自己管理 | 内置数据集管理 |\n",
        "| **实验追踪** | 需要自己实现 | 内置实验追踪 |\n",
        "| **成本** | 主要是LLM调用成本 | LLM调用 + 平台费用 |\n",
        "| **适用场景** | 快速评估、本地开发 | 生产环境、团队协作 |\n",
        "\n",
        "**选择建议**：\n",
        "- 开发阶段：可以使用RAGAS进行快速评估\n",
        "- 生产环境：建议使用LangSmith进行持续监控\n",
        "- 团队协作：LangSmith提供更好的协作功能\n",
        "\n",
        "### 8.5 常见问题\n",
        "\n",
        "**Q1: LangSmith评估需要多长时间？**\n",
        "- A: 评估时间取决于数据集大小和LLM响应时间。LangSmith支持并发评估以提高效率。\n",
        "\n",
        "**Q2: 如何降低评估成本？**\n",
        "- A: 使用较便宜的LLM模型（如GPT-3.5）作为评估器；减少评估频率；使用采样评估。\n",
        "\n",
        "**Q3: 评估结果不稳定怎么办？**\n",
        "- A: 使用temperature=0确保一致性；多次评估取平均值；检查评估数据质量。\n",
        "\n",
        "**Q4: 可以自定义评估指标吗？**\n",
        "- A: 可以，LangSmith完全支持自定义评估器，你可以根据需求编写任何评估逻辑。\n",
        "\n",
        "**Q5: 如何在生产环境中使用？**\n",
        "- A: 将LangSmith追踪集成到RAG系统中，设置定期评估任务，监控关键指标。\n",
        "\n",
        "---\n",
        "\n",
        "## 总结\n",
        "\n",
        "本教程介绍了如何使用LangSmith平台评估RAG系统的向量检索性能。通过Correctness、Groundedness、Relevance、Retrieval Precision和Retrieval Recall等评估器，我们可以全面了解RAG系统在检索和生成方面的表现。\n",
        "\n",
        "**关键要点**：\n",
        "- ✅ LangSmith提供完整的评估平台和可视化界面\n",
        "- ✅ 支持多种评估器类型，可以灵活组合\n",
        "- ✅ 内置数据集管理和实验追踪功能\n",
        "- ✅ 适合生产环境的持续监控和优化\n",
        "\n",
        "**下一步**：\n",
        "- 在LangSmith平台创建账户并获取API密钥\n",
        "- 将评估集成到你的RAG系统中\n",
        "- 建立评估数据集和定期评估机制\n",
        "- 根据评估结果持续优化系统性能\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
