{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAGAS 向量检索评估教程\n",
        "\n",
        "## 目录\n",
        "1. [RAGAS框架概述](#概述)\n",
        "2. [安装和环境配置](#安装)\n",
        "3. [RAGAS评估指标详解](#指标详解)\n",
        "4. [创建RAGAS评估链](#评估链)\n",
        "5. [单个结果评估](#单个评估)\n",
        "6. [批量评估实战](#批量评估)\n",
        "7. [向量检索专项评估](#检索评估)\n",
        "8. [完整示例代码](#完整示例)\n",
        "9. [结果分析和优化建议](#分析优化)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. RAGAS框架概述 {#概述}\n",
        "\n",
        "RAGAS（Retrieval Augmented Generation Assessment Suite）是一个专门为RAG系统设计的评估框架。\n",
        "\n",
        "### 核心特点：\n",
        "- ✅ **无参考评估**：不需要标准答案就能评估RAG系统\n",
        "- ✅ **多维度指标**：覆盖检索、生成、端到端性能\n",
        "- ✅ **LangChain集成**：可以与LangChain无缝集成\n",
        "- ✅ **自动化评估**：使用LLM自动评估，减少人工标注成本\n",
        "\n",
        "### RAGAS评估流程：\n",
        "```\n",
        "用户问题 → RAG系统 → 检索文档 + 生成答案\n",
        "                          ↓\n",
        "                    RAGAS评估器\n",
        "                          ↓\n",
        "              Faithfulness + Answer Relevancy\n",
        "              + Context Precision + Context Recall\n",
        "                          ↓\n",
        "                    评估报告\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 配置环境变量（如果使用OpenAI）\n",
        "import os\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
        "import langchain_core\n",
        "import langchain_openai\n",
        "import ragas\n",
        "\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall\n",
        ")\n",
        "from ragas import evaluate, Dataset\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. RAGAS评估指标详解 {#指标详解}\n",
        "\n",
        "RAGAS提供了四个核心评估指标：\n",
        "\n",
        "### 3.1 Faithfulness（忠实度）\n",
        "**定义**：评估生成的答案是否忠实于检索到的文档，即答案是否基于文档内容生成，没有引入幻觉。\n",
        "\n",
        "**评估方法**：\n",
        "1. 从答案中提取可验证的事实声明\n",
        "2. 检查这些声明是否可以从检索文档中推断出来\n",
        "3. 计算可验证声明的比例\n",
        "\n",
        "**取值范围**：0-1，越接近1表示答案越忠实于文档\n",
        "\n",
        "### 3.2 Answer Relevancy（答案相关性）\n",
        "**定义**：评估生成的答案与用户问题的相关性。\n",
        "\n",
        "**评估方法**：\n",
        "1. 从答案中提取关键信息\n",
        "2. 评估这些信息回答用户问题的程度\n",
        "3. 考虑答案的完整性和相关性\n",
        "\n",
        "**取值范围**：0-1，越接近1表示答案越相关\n",
        "\n",
        "### 3.3 Context Precision（上下文精确度）\n",
        "**定义**：评估检索到的文档中有多少是真正相关的。\n",
        "\n",
        "**评估方法**：\n",
        "1. 从答案中提取相关信息\n",
        "2. 评估每个检索文档对这些信息的相关性\n",
        "3. 计算相关文档在前K个结果中的比例\n",
        "\n",
        "**取值范围**：0-1，越接近1表示检索越精确\n",
        "\n",
        "### 3.4 Context Recall（上下文召回率）\n",
        "**定义**：评估所有相关文档中有多少被成功检索到。\n",
        "\n",
        "**评估方法**：\n",
        "1. 使用标准答案（ground truth）作为参考\n",
        "2. 评估检索到的文档是否包含标准答案中的信息\n",
        "3. 计算覆盖的信息比例\n",
        "\n",
        "**取值范围**：0-1，越接近1表示召回越完整\n",
        "\n",
        "**注意**：Context Recall需要ground truth作为参考，其他三个指标是无需参考的。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 创建RAGAS评估指标 {#评估链}\n",
        "\n",
        "RAGAS新版本（0.1.0+）使用`evaluate()`函数和`Dataset`来进行评估，不再使用`RagasEvaluatorChain`。我们可以直接使用评估指标和`evaluate()`函数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定义评估指标列表\n",
        "# 新版本的RAGAS直接使用evaluate()函数，不再需要创建评估链\n",
        "\n",
        "# 确保导入已执行（如果未导入，尝试导入）\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall\n",
        ")\n",
        "from ragas import evaluate, Dataset\n",
        "\n",
        "metrics = [\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall\n",
        "]\n",
        "\n",
        "# 使用列表索引来映射指标对象到名称（因为指标对象不可哈希，不能用作字典键）\n",
        "metric_names_list = [\n",
        "    \"faithfulness\",\n",
        "    \"answer_relevancy\",\n",
        "    \"context_precision\",\n",
        "    \"context_recall\"\n",
        "]\n",
        "\n",
        "# 创建一个辅助函数来获取指标名称\n",
        "def get_metric_name(metric):\n",
        "    \"\"\"根据指标对象获取其名称\"\"\"\n",
        "    try:\n",
        "        idx = metrics.index(metric)\n",
        "        return metric_names_list[idx]\n",
        "    except ValueError:\n",
        "        # 如果找不到，尝试使用字符串表示\n",
        "        return str(metric).lower().replace(\" \", \"_\")\n",
        "\n",
        "print(\"已准备以下评估指标：\")\n",
        "for i, metric in enumerate(metrics):\n",
        "    print(f\"  - {metric_names_list[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 单个结果评估 {#单个评估}\n",
        "\n",
        "首先，我们需要创建一个简单的RAG系统作为示例，然后对其进行评估。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用 HuggingFace Embeddings（更稳定，避免 modelscope 依赖问题）\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def create_simple_rag_system():\n",
        "    \"\"\"\n",
        "    创建一个简单的RAG系统，使用 HuggingFace 的中文 embedding 模型\n",
        "    在实际使用中，你需要替换为你的RAG系统\n",
        "    \"\"\"\n",
        "    # 示例：使用OpenAI模型\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\", api_key='', base_url='', temperature=0, max_tokens=1000)\n",
        "    \n",
        "    # 使用 HuggingFace 的 BGE-M3 中文模型（自动下载到本地）\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-zh-v1.5\",  # 中文小模型，速度快\n",
        "        model_kwargs={'device': 'cpu'},\n",
        "        encode_kwargs={'normalize_embeddings': True}\n",
        "    )\n",
        "\n",
        "    # 创建示例文档（实际应该从向量库加载）\n",
        "    example_docs = [\n",
        "        \"RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。\",\n",
        "        \"RAG的核心思想是在生成答案前，先从知识库中检索相关文档。\",\n",
        "        \"RAG可以解决大模型的知识盲区问题，让模型能够访问非公开知识。\"\n",
        "    ]\n",
        "    \n",
        "    # 创建模板\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    template = \"\"\"\n",
        "    根据以下文档回答问题：\n",
        "\n",
        "    文档：\n",
        "    {context}\n",
        "\n",
        "    问题：{question}\n",
        "\n",
        "    答案：\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "    \n",
        "    # 返回一个简单的RAG函数\n",
        "    def rag_bot(question: str, documents: list = None):\n",
        "        if documents is None:\n",
        "            documents = example_docs\n",
        "        \n",
        "        context = \"\\n\".join(documents)\n",
        "        # 实际应用中，这里应该用 LLM 和 embedding 检索\n",
        "        # answer = llm.invoke(prompt.format(context=context, question=question))\n",
        "        answer = \"RAG是一种结合检索和生成的技术，它通过在生成答案前从知识库检索相关文档来解决大模型的知识盲区问题。\"\n",
        "\n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"contexts\": documents,\n",
        "            \"ground_truths\": [answer]  # 示例ground truth\n",
        "        }\n",
        "    \n",
        "    return rag_bot\n",
        "\n",
        "# 创建RAG系统\n",
        "rag_bot = create_simple_rag_system()\n",
        "print(\"===== RAG系统已创建（使用 HuggingFace BGE 中文模型）=====\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 执行单个查询并获取结果\n",
        "question = \"RAG是什么？\"\n",
        "result = rag_bot(question)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"RAG系统输出：\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"问题：{result['question']}\")\n",
        "print(f\"\\n答案：{result['answer']}\")\n",
        "print(f\"\\n检索到的文档数量：{len(result['contexts'])}\")\n",
        "print(f\"\\n检索文档：\")\n",
        "for i, doc in enumerate(result['contexts'], 1):\n",
        "    print(f\"  {i}. {doc[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用RAGAS评估单个结果\n",
        "print(\"=\" * 50)\n",
        "print(\"RAGAS评估结果：\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 新版本的RAGAS使用Dataset和evaluate()函数\n",
        "# 准备评估数据：Dataset需要包含question, answer, contexts, ground_truths字段\n",
        "eval_data = {\n",
        "    \"question\": [result[\"question\"]],\n",
        "    \"answer\": [result[\"answer\"]],\n",
        "    \"contexts\": [result[\"contexts\"]],\n",
        "    \"ground_truths\": [result.get(\"ground_truths\", [result[\"answer\"]])]\n",
        "}\n",
        "\n",
        "# 创建Dataset对象\n",
        "dataset = Dataset.from_dict(eval_data)\n",
        "\n",
        "# 执行评估（包含所有指标）\n",
        "# 注意：context_recall需要ground_truths，其他指标不需要\n",
        "evaluation_result = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "# 提取评估结果\n",
        "print(\"\\n评估分数：\")\n",
        "evaluation_results = {}\n",
        "for metric in metrics:\n",
        "    metric_name = get_metric_name(metric)  # 使用辅助函数获取指标名称\n",
        "    try:\n",
        "        score = evaluation_result[metric_name]\n",
        "        evaluation_results[metric_name] = score\n",
        "        print(f\"{metric_name:20s}: {score:.4f}\" if isinstance(score, (int, float)) else f\"{metric_name:20s}: {score}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{metric_name:20s}: 评估失败 - {str(e)}\")\n",
        "        evaluation_results[metric_name] = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 批量评估实战 {#批量评估}\n",
        "\n",
        "在实际应用中，我们需要对多个测试用例进行批量评估，以全面了解RAG系统的性能。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备测试数据集\n",
        "test_dataset = [\n",
        "    {\n",
        "        \"question\": \"RAG是什么？\",\n",
        "        \"ground_truths\": [\"RAG是一种结合检索和生成的技术，用于解决大模型的知识盲区问题。\"],\n",
        "        \"relevant_docs\": [\n",
        "            \"RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。\",\n",
        "            \"RAG的核心思想是在生成答案前，先从知识库中检索相关文档。\",\n",
        "            \"RAG可以解决大模型的知识盲区问题，让模型能够访问非公开知识。\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"RAG的核心思想是什么？\",\n",
        "        \"ground_truths\": [\"RAG的核心思想是在生成答案前先从知识库检索相关文档。\"],\n",
        "        \"relevant_docs\": [\n",
        "            \"RAG的核心思想是在生成答案前，先从知识库中检索相关文档。\",\n",
        "            \"检索到的文档作为上下文输入给生成模型，帮助生成更准确的答案。\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"RAG能解决什么问题？\",\n",
        "        \"ground_truths\": [\"RAG可以解决大模型的知识盲区问题，让模型能够访问非公开知识。\"],\n",
        "        \"relevant_docs\": [\n",
        "            \"RAG可以解决大模型的知识盲区问题，让模型能够访问非公开知识。\",\n",
        "            \"通过RAG，模型可以动态检索最新的信息，而不需要重新训练。\"\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"测试数据集大小：{len(test_dataset)}\")\n",
        "print(\"\\n数据集示例：\")\n",
        "for i, item in enumerate(test_dataset[:2], 1):\n",
        "    print(f\"\\n{i}. 问题：{item['question']}\")\n",
        "    print(f\"   相关文档数：{len(item['relevant_docs'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 批量评估函数\n",
        "def batch_evaluate_ragas(rag_system, test_dataset, metrics_list):\n",
        "    \"\"\"\n",
        "    批量评估RAG系统\n",
        "    \n",
        "    Args:\n",
        "        rag_system: RAG系统函数\n",
        "        test_dataset: 测试数据集列表\n",
        "        metrics_list: 评估指标列表\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame: 包含所有评估结果的DataFrame\n",
        "    \"\"\"\n",
        "    # 收集所有RAG系统输出\n",
        "    all_questions = []\n",
        "    all_answers = []\n",
        "    all_contexts = []\n",
        "    all_ground_truths = []\n",
        "    \n",
        "    print(\"收集RAG系统输出...\")\n",
        "    for idx, test_case in enumerate(test_dataset):\n",
        "        print(f\"  处理测试用例 {idx + 1}/{len(test_dataset)}: {test_case['question']}\")\n",
        "        \n",
        "        # 获取RAG系统输出\n",
        "        result = rag_system(\n",
        "            question=test_case[\"question\"],\n",
        "            documents=test_case.get(\"relevant_docs\", [])\n",
        "        )\n",
        "        \n",
        "        all_questions.append(result[\"question\"])\n",
        "        all_answers.append(result[\"answer\"])\n",
        "        all_contexts.append(result[\"contexts\"])\n",
        "        all_ground_truths.append(test_case.get(\"ground_truths\", [result[\"answer\"]]))\n",
        "    \n",
        "    # 创建Dataset对象\n",
        "    print(\"\\n创建评估数据集...\")\n",
        "    eval_data = {\n",
        "        \"question\": all_questions,\n",
        "        \"answer\": all_answers,\n",
        "        \"contexts\": all_contexts,\n",
        "        \"ground_truths\": all_ground_truths\n",
        "    }\n",
        "    dataset = Dataset.from_dict(eval_data)\n",
        "    \n",
        "    # 执行批量评估\n",
        "    print(\"执行评估...\")\n",
        "    evaluation_result = evaluate(\n",
        "        dataset=dataset,\n",
        "        metrics=metrics_list\n",
        "    )\n",
        "    \n",
        "    # 转换为DataFrame格式\n",
        "    results = []\n",
        "    for idx in range(len(test_dataset)):\n",
        "        case_result = {\n",
        "            \"question\": all_questions[idx],\n",
        "            \"answer\": all_answers[idx]\n",
        "        }\n",
        "        \n",
        "        # 提取每个指标的分数\n",
        "        for metric in metrics_list:\n",
        "            metric_name = get_metric_name(metric)  # 使用辅助函数获取指标名称\n",
        "            try:\n",
        "                # 评估结果是一个字典，每个指标对应一个数组\n",
        "                scores = evaluation_result[metric_name]\n",
        "                case_result[metric_name] = scores[idx] if idx < len(scores) else None\n",
        "            except Exception as e:\n",
        "                print(f\"  提取 {metric_name} 分数时出错: {str(e)}\")\n",
        "                case_result[metric_name] = None\n",
        "        \n",
        "        results.append(case_result)\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# 执行批量评估\n",
        "print(\"开始批量评估...\")\n",
        "evaluation_df = batch_evaluate_ragas(rag_bot, test_dataset, metrics)\n",
        "print(\"\\n评估完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 查看评估结果\n",
        "print(\"=\" * 80)\n",
        "print(\"批量评估结果：\")\n",
        "print(\"=\" * 80)\n",
        "print(evaluation_df.to_string(index=False))\n",
        "\n",
        "# 计算平均分数\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"平均评估分数：\")\n",
        "print(\"=\" * 80)\n",
        "metric_columns = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
        "for metric in metric_columns:\n",
        "    if metric in evaluation_df.columns:\n",
        "        avg_score = evaluation_df[metric].mean()\n",
        "        print(f\"{metric:20s}: {avg_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化评估结果\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # 支持中文\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 绘制评估分数对比图\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 1. 各指标平均分数柱状图\n",
        "metric_means = []\n",
        "metric_names = []\n",
        "for metric in metric_columns:\n",
        "    if metric in evaluation_df.columns:\n",
        "        metric_means.append(evaluation_df[metric].mean())\n",
        "        metric_names.append(metric.replace('_', ' ').title())\n",
        "\n",
        "axes[0].bar(metric_names, metric_means, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[0].set_title('RAGAS评估指标平均分数', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('分数', fontsize=12)\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 添加数值标签\n",
        "for i, v in enumerate(metric_means):\n",
        "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# 2. 每个测试用例的分数趋势\n",
        "x = range(len(evaluation_df))\n",
        "for metric in metric_columns:\n",
        "    if metric in evaluation_df.columns:\n",
        "        axes[1].plot(x, evaluation_df[metric], marker='o', label=metric.replace('_', ' ').title(), linewidth=2)\n",
        "\n",
        "axes[1].set_title('各测试用例评估分数趋势', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('测试用例编号', fontsize=12)\n",
        "axes[1].set_ylabel('分数', fontsize=12)\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels([f'Case {i+1}' for i in x])\n",
        "axes[1].legend(loc='best')\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 向量检索专项评估 {#检索评估}\n",
        "\n",
        "对于向量检索系统，Context Precision和Context Recall是最重要的指标。本节详细介绍如何使用这两个指标评估检索质量。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 专门用于评估检索质量的函数\n",
        "def evaluate_retrieval_quality(\n",
        "    questions: list,\n",
        "    retrieval_function,\n",
        "    ground_truth_docs: list,\n",
        "    k: int = 5\n",
        "):\n",
        "    \"\"\"\n",
        "    评估向量检索质量\n",
        "    \n",
        "    Args:\n",
        "        questions: 问题列表\n",
        "        retrieval_function: 检索函数，输入question，返回文档列表\n",
        "        ground_truth_docs: 每个问题对应的真实相关文档列表\n",
        "        k: 评估前k个检索结果\n",
        "    \n",
        "    Returns:\n",
        "        dict: 包含Context Precision和Context Recall的评估结果\n",
        "    \"\"\"\n",
        "    # 收集所有检索结果\n",
        "    all_questions = []\n",
        "    all_contexts = []\n",
        "    all_ground_truths = []\n",
        "    all_answers = []  # 对于检索评估，答案可以为空或占位符\n",
        "    \n",
        "    for question, true_docs in zip(questions, ground_truth_docs):\n",
        "        # 执行检索\n",
        "        retrieved_docs = retrieval_function(question)[:k]\n",
        "        \n",
        "        all_questions.append(question)\n",
        "        all_contexts.append(retrieved_docs)\n",
        "        all_ground_truths.append(true_docs)\n",
        "        all_answers.append(\"\")  # 检索评估不需要实际答案\n",
        "    \n",
        "    # 创建Dataset对象\n",
        "    eval_data = {\n",
        "        \"question\": all_questions,\n",
        "        \"answer\": all_answers,\n",
        "        \"contexts\": all_contexts,\n",
        "        \"ground_truths\": all_ground_truths\n",
        "    }\n",
        "    dataset = Dataset.from_dict(eval_data)\n",
        "    \n",
        "    # 只评估检索相关的指标\n",
        "    retrieval_metrics = [context_precision, context_recall]\n",
        "    \n",
        "    # 执行评估\n",
        "    evaluation_result = evaluate(\n",
        "        dataset=dataset,\n",
        "        metrics=retrieval_metrics\n",
        "    )\n",
        "    \n",
        "    # 提取结果\n",
        "    precision_scores = evaluation_result[\"context_precision\"]\n",
        "    recall_scores = evaluation_result[\"context_recall\"]\n",
        "    \n",
        "    return {\n",
        "        \"context_precision\": {\n",
        "            \"scores\": precision_scores.tolist() if hasattr(precision_scores, 'tolist') else list(precision_scores),\n",
        "            \"mean\": np.mean(precision_scores),\n",
        "            \"std\": np.std(precision_scores)\n",
        "        },\n",
        "        \"context_recall\": {\n",
        "            \"scores\": recall_scores.tolist() if hasattr(recall_scores, 'tolist') else list(recall_scores),\n",
        "            \"mean\": np.mean(recall_scores),\n",
        "            \"std\": np.std(recall_scores)\n",
        "        }\n",
        "    }\n",
        "\n",
        "# 示例：创建检索函数\n",
        "def simple_retrieval(question: str):\n",
        "    \"\"\"简单的检索函数示例\"\"\"\n",
        "    # 实际应该从向量库检索\n",
        "    # 这里返回示例文档\n",
        "    all_docs = [\n",
        "        \"RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。\",\n",
        "        \"RAG的核心思想是在生成答案前，先从知识库中检索相关文档。\",\n",
        "        \"RAG可以解决大模型的知识盲区问题，让模型能够访问非公开知识。\",\n",
        "        \"向量检索是RAG系统中常用的检索方法。\",\n",
        "        \"检索到的文档作为上下文输入给生成模型。\"\n",
        "    ]\n",
        "    # 简单模拟：根据关键词匹配返回文档\n",
        "    if \"RAG\" in question:\n",
        "        return all_docs[:3]\n",
        "    else:\n",
        "        return all_docs[:2]\n",
        "\n",
        "# 执行检索质量评估\n",
        "questions = [\"RAG是什么？\", \"什么是向量检索？\"]\n",
        "ground_truths = [\n",
        "    [\"RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。\",\n",
        "     \"RAG的核心思想是在生成答案前，先从知识库中检索相关文档。\"],\n",
        "    [\"向量检索是RAG系统中常用的检索方法。\"]\n",
        "]\n",
        "\n",
        "retrieval_results = evaluate_retrieval_quality(\n",
        "    questions=questions,\n",
        "    retrieval_function=simple_retrieval,\n",
        "    ground_truth_docs=ground_truths,\n",
        "    k=5\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"检索质量评估结果：\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nContext Precision:\")\n",
        "print(f\"  平均分数: {retrieval_results['context_precision']['mean']:.4f}\")\n",
        "print(f\"  标准差: {retrieval_results['context_precision']['std']:.4f}\")\n",
        "print(f\"  各用例分数: {retrieval_results['context_precision']['scores']}\")\n",
        "\n",
        "print(f\"\\nContext Recall:\")\n",
        "print(f\"  平均分数: {retrieval_results['context_recall']['mean']:.4f}\")\n",
        "print(f\"  标准差: {retrieval_results['context_recall']['std']:.4f}\")\n",
        "print(f\"  各用例分数: {retrieval_results['context_recall']['scores']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 完整示例代码 {#完整示例}\n",
        "\n",
        "以下是一个完整的RAG系统评估示例，展示了如何评估一个基于FAISS向量库的RAG系统。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 完整的RAG系统评估示例\n",
        "# 注意：这是一个模板，需要根据你的实际RAG系统进行调整\n",
        "\n",
        "class CompleteRAGEvaluator:\n",
        "    \"\"\"完整的RAG评估器（使用新版本RAGAS API）\"\"\"\n",
        "    \n",
        "    def __init__(self, rag_chain, vector_store, llm=None):\n",
        "        \"\"\"\n",
        "        初始化评估器\n",
        "        \n",
        "        Args:\n",
        "            rag_chain: LangChain RAG链\n",
        "            vector_store: 向量存储（FAISS等）\n",
        "            llm: LLM模型（可选，用于生成答案）\n",
        "        \"\"\"\n",
        "        self.rag_chain = rag_chain\n",
        "        self.vector_store = vector_store\n",
        "        self.llm = llm\n",
        "        \n",
        "        # 定义评估指标（新版本不再使用评估链）\n",
        "        self.metrics = [\n",
        "            faithfulness,\n",
        "            answer_relevancy,\n",
        "            context_precision,\n",
        "            context_recall\n",
        "        ]\n",
        "    \n",
        "    def query(self, question: str, k: int = 5):\n",
        "        \"\"\"\n",
        "        执行RAG查询\n",
        "        \n",
        "        Args:\n",
        "            question: 用户问题\n",
        "            k: 检索文档数量\n",
        "        \n",
        "        Returns:\n",
        "            dict: 包含question, answer, contexts的字典\n",
        "        \"\"\"\n",
        "        # 检索相关文档\n",
        "        if hasattr(self.vector_store, 'as_retriever'):\n",
        "            retriever = self.vector_store.as_retriever(search_kwargs={\"k\": k})\n",
        "            contexts = retriever.get_relevant_documents(question)\n",
        "        else:\n",
        "            contexts = self.vector_store.similarity_search(question, k=k)\n",
        "        \n",
        "        # 提取文档内容\n",
        "        contexts_text = [doc.page_content if hasattr(doc, 'page_content') else str(doc) \n",
        "                        for doc in contexts]\n",
        "        \n",
        "        # 生成答案\n",
        "        if self.rag_chain:\n",
        "            answer = self.rag_chain.invoke(question)\n",
        "            if hasattr(answer, 'content'):\n",
        "                answer = answer.content\n",
        "        else:\n",
        "            answer = \"示例答案\"  # 如果没有提供链，返回示例答案\n",
        "        \n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"contexts\": contexts_text\n",
        "        }\n",
        "    \n",
        "    def evaluate_single(self, question: str, ground_truths: list = None, k: int = 5):\n",
        "        \"\"\"\n",
        "        评估单个查询\n",
        "        \n",
        "        Args:\n",
        "            question: 用户问题\n",
        "            ground_truths: 标准答案列表（用于Context Recall）\n",
        "            k: 检索文档数量\n",
        "        \n",
        "        Returns:\n",
        "            dict: 评估结果\n",
        "        \"\"\"\n",
        "        # 执行查询\n",
        "        result = self.query(question, k=k)\n",
        "        \n",
        "        # 准备评估数据\n",
        "        eval_data = {\n",
        "            \"question\": [result[\"question\"]],\n",
        "            \"answer\": [result[\"answer\"]],\n",
        "            \"contexts\": [result[\"contexts\"]],\n",
        "            \"ground_truths\": [ground_truths if ground_truths else [result[\"answer\"]]]\n",
        "        }\n",
        "        dataset = Dataset.from_dict(eval_data)\n",
        "        \n",
        "        # 执行评估\n",
        "        evaluation_result = evaluate(\n",
        "            dataset=dataset,\n",
        "            metrics=self.metrics\n",
        "        )\n",
        "        \n",
        "        # 提取评估结果\n",
        "        eval_result = {\n",
        "            \"question\": result[\"question\"],\n",
        "            \"answer\": result[\"answer\"],\n",
        "            \"contexts\": result[\"contexts\"]\n",
        "        }\n",
        "        \n",
        "        for metric in self.metrics:\n",
        "            metric_name = get_metric_name(metric)  # 使用辅助函数获取指标名称\n",
        "            try:\n",
        "                score = evaluation_result[metric_name]\n",
        "                eval_result[metric_name] = score[0] if isinstance(score, (list, np.ndarray)) else score\n",
        "            except Exception as e:\n",
        "                print(f\"评估 {metric_name} 时出错: {str(e)}\")\n",
        "                eval_result[metric_name] = None\n",
        "        \n",
        "        return eval_result\n",
        "    \n",
        "    def evaluate_batch(self, test_dataset: list, k: int = 5):\n",
        "        \"\"\"\n",
        "        批量评估\n",
        "        \n",
        "        Args:\n",
        "            test_dataset: 测试数据集，每个元素包含question和ground_truths\n",
        "            k: 检索文档数量\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame: 评估结果DataFrame\n",
        "        \"\"\"\n",
        "        # 收集所有查询结果\n",
        "        all_questions = []\n",
        "        all_answers = []\n",
        "        all_contexts = []\n",
        "        all_ground_truths = []\n",
        "        \n",
        "        for test_case in test_dataset:\n",
        "            result = self.query(test_case[\"question\"], k=k)\n",
        "            all_questions.append(result[\"question\"])\n",
        "            all_answers.append(result[\"answer\"])\n",
        "            all_contexts.append(result[\"contexts\"])\n",
        "            all_ground_truths.append(test_case.get(\"ground_truths\", [result[\"answer\"]]))\n",
        "        \n",
        "        # 创建Dataset\n",
        "        eval_data = {\n",
        "            \"question\": all_questions,\n",
        "            \"answer\": all_answers,\n",
        "            \"contexts\": all_contexts,\n",
        "            \"ground_truths\": all_ground_truths\n",
        "        }\n",
        "        dataset = Dataset.from_dict(eval_data)\n",
        "        \n",
        "        # 执行批量评估\n",
        "        evaluation_result = evaluate(\n",
        "            dataset=dataset,\n",
        "            metrics=self.metrics\n",
        "        )\n",
        "        \n",
        "        # 转换为DataFrame\n",
        "        results = []\n",
        "        for idx in range(len(test_dataset)):\n",
        "            case_result = {\n",
        "                \"question\": all_questions[idx],\n",
        "                \"answer\": all_answers[idx]\n",
        "            }\n",
        "            \n",
        "            for metric in self.metrics:\n",
        "                metric_name = metric_names[metric]\n",
        "                try:\n",
        "                    scores = evaluation_result[metric_name]\n",
        "                    case_result[metric_name] = scores[idx] if idx < len(scores) else None\n",
        "                except Exception as e:\n",
        "                    print(f\"提取 {metric_name} 分数时出错: {str(e)}\")\n",
        "                    case_result[metric_name] = None\n",
        "            \n",
        "            results.append(case_result)\n",
        "        \n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "# 使用示例\n",
        "print(\"完整RAG评估器已创建\")\n",
        "print(\"\\n使用方法：\")\n",
        "print(\"1. 初始化评估器：evaluator = CompleteRAGEvaluator(rag_chain, vector_store)\")\n",
        "print(\"2. 单个评估：result = evaluator.evaluate_single(question, ground_truths)\")\n",
        "print(\"3. 批量评估：results_df = evaluator.evaluate_batch(test_dataset)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_evaluation_results(evaluation_df):\n",
        "    \"\"\"\n",
        "    分析评估结果并给出优化建议\n",
        "    \n",
        "    Args:\n",
        "        evaluation_df: 包含评估结果的DataFrame\n",
        "    \n",
        "    Returns:\n",
        "        dict: 分析结果和建议\n",
        "    \"\"\"\n",
        "    analysis = {\n",
        "        \"summary\": {},\n",
        "        \"recommendations\": []\n",
        "    }\n",
        "    \n",
        "    thresholds = {\n",
        "        \"faithfulness\": 0.7,\n",
        "        \"answer_relevancy\": 0.7,\n",
        "        \"context_precision\": 0.7,\n",
        "        \"context_recall\": 0.7\n",
        "    }\n",
        "    \n",
        "    for metric, threshold in thresholds.items():\n",
        "        if metric in evaluation_df.columns:\n",
        "            mean_score = evaluation_df[metric].mean()\n",
        "            analysis[\"summary\"][metric] = {\n",
        "                \"mean\": mean_score,\n",
        "                \"threshold\": threshold,\n",
        "                \"status\": \"良好\" if mean_score >= threshold else \"需要改进\"\n",
        "            }\n",
        "            \n",
        "            if mean_score < threshold:\n",
        "                if metric == \"faithfulness\":\n",
        "                    analysis[\"recommendations\"].append(\n",
        "                        \"Faithfulness得分较低：改进prompt，明确要求基于文档回答，避免幻觉\"\n",
        "                    )\n",
        "                elif metric == \"answer_relevancy\":\n",
        "                    analysis[\"recommendations\"].append(\n",
        "                        \"Answer Relevancy得分较低：改进问题理解，优化prompt中的回答要求\"\n",
        "                    )\n",
        "                elif metric == \"context_precision\":\n",
        "                    analysis[\"recommendations\"].append(\n",
        "                        \"Context Precision得分较低：改进embedding模型，增加rerank步骤\"\n",
        "                    )\n",
        "                elif metric == \"context_recall\":\n",
        "                    analysis[\"recommendations\"].append(\n",
        "                        \"Context Recall得分较低：增加检索数量k，使用混合检索策略\"\n",
        "                    )\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "# 如果有评估结果，进行分析\n",
        "if 'evaluation_df' in locals() and not evaluation_df.empty:\n",
        "    analysis = analyze_evaluation_results(evaluation_df)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"评估结果分析：\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"\\n指标概览：\")\n",
        "    for metric, info in analysis[\"summary\"].items():\n",
        "        status_icon = \"✅\" if info[\"status\"] == \"良好\" else \"⚠️\"\n",
        "        print(f\"{status_icon} {metric:20s}: {info['mean']:.4f} (阈值: {info['threshold']:.2f}) - {info['status']}\")\n",
        "    \n",
        "    if analysis[\"recommendations\"]:\n",
        "        print(\"\\n优化建议：\")\n",
        "        for i, rec in enumerate(analysis[\"recommendations\"], 1):\n",
        "            print(f\"{i}. {rec}\")\n",
        "    else:\n",
        "        print(\"\\n✅ 所有指标都达到阈值，系统表现良好！\")\n",
        "else:\n",
        "    print(\"请先运行批量评估以生成分析结果\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 最佳实践总结\n",
        "\n",
        "1. **评估数据集构建**：\n",
        "   - 包含不同类型的问题（简单事实、复杂推理、多跳问题）\n",
        "   - 确保有足够的ground truth文档用于Context Recall评估\n",
        "   - 定期更新评估数据集\n",
        "\n",
        "2. **评估频率**：\n",
        "   - 每次模型或prompt更新后都应重新评估\n",
        "   - 建立定期评估机制（如每周或每月）\n",
        "   - 记录评估历史，追踪改进效果\n",
        "\n",
        "3. **指标选择**：\n",
        "   - 根据应用场景选择重点关注的指标\n",
        "   - 检索系统重点关注Context Precision和Recall\n",
        "   - 生成系统重点关注Faithfulness和Answer Relevancy\n",
        "\n",
        "4. **优化迭代**：\n",
        "   - 根据评估结果定位问题\n",
        "   - 逐步优化（先优化检索，再优化生成）\n",
        "   - 使用A/B测试验证优化效果\n",
        "\n",
        "### 9.4 常见问题\n",
        "\n",
        "**Q1: RAGAS评估需要多长时间？**\n",
        "- A: 每个评估需要调用LLM，速度取决于LLM的响应时间。批量评估可以并行处理以提高效率。\n",
        "\n",
        "**Q2: 评估结果不稳定怎么办？**\n",
        "- A: 使用temperature=0确保一致性；多次评估取平均值；检查评估数据质量。\n",
        "\n",
        "**Q3: 如何提高评估准确性？**\n",
        "- A: 使用更强大的LLM作为评估器（如GPT-4）；确保ground truth质量；增加评估数据量。\n",
        "\n",
        "**Q4: 可以自定义评估指标吗？**\n",
        "- A: RAGAS提供了基础的评估框架，可以通过LangChain自定义评估链来实现特定的评估逻辑。\n",
        "\n",
        "---\n",
        "\n",
        "## 总结\n",
        "\n",
        "本教程介绍了如何使用RAGAS框架评估RAG系统的向量检索性能。通过Faithfulness、Answer Relevancy、Context Precision和Context Recall四个核心指标，我们可以全面了解RAG系统在检索和生成方面的表现。\n",
        "\n",
        "**关键要点**：\n",
        "- ✅ RAGAS提供了无参考评估能力，降低评估成本\n",
        "- ✅ 四个核心指标覆盖检索和生成的关键维度\n",
        "- ✅ 可以与LangChain无缝集成\n",
        "- ✅ 支持单个和批量评估\n",
        "\n",
        "**下一步**：\n",
        "- 将RAGAS评估集成到你的RAG系统中\n",
        "- 建立评估数据集和定期评估机制\n",
        "- 根据评估结果持续优化系统性能\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
