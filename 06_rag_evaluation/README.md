# ç¬¬äº”æ­¥ï¼šRAGæ•ˆæœè¯„ä¼° ğŸ“Š

## æ¨¡å—æ¦‚è¿°

æœ¬æ¨¡å—ä»‹ç»å¦‚ä½•ç³»ç»Ÿåœ°è¯„ä¼°å’Œä¼˜åŒ–RAGç³»ç»Ÿçš„æ•ˆæœã€‚è¯„ä¼°æ˜¯æŒç»­æ”¹è¿›çš„åŸºç¡€ï¼Œå¥½çš„è¯„ä¼°ä½“ç³»èƒ½å¤ŸæŒ‡å¯¼æˆ‘ä»¬æ‰¾åˆ°ç³»ç»Ÿçš„ç“¶é¢ˆå’Œä¼˜åŒ–æ–¹å‘ã€‚

## å­¦ä¹ ç›®æ ‡

- âœ… ç†è§£RAGè¯„ä¼°çš„å…³é”®æŒ‡æ ‡
- âœ… æŒæ¡æ‰‹åŠ¨è¯„ä¼°å’Œè‡ªåŠ¨è¯„ä¼°æ–¹æ³•
- âœ… å­¦ä¼šä½¿ç”¨è¯„ä¼°å·¥å…·å’Œæ¡†æ¶
- âœ… å»ºç«‹æŒç»­ä¼˜åŒ–çš„è¯„ä¼°æµç¨‹

## RAGè¯„ä¼°çš„ä¸‰ä¸ªå±‚æ¬¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. æ£€ç´¢è¯„ä¼°ï¼ˆRetrievalï¼‰           â”‚
â”‚  èƒ½å¦æ£€ç´¢åˆ°æ­£ç¡®çš„æ–‡æ¡£ï¼Ÿ              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. ç”Ÿæˆè¯„ä¼°ï¼ˆGenerationï¼‰          â”‚
â”‚  ç­”æ¡ˆæ˜¯å¦å‡†ç¡®ã€å®Œæ•´ã€ç›¸å…³ï¼Ÿ          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. ç«¯åˆ°ç«¯è¯„ä¼°ï¼ˆEnd-to-Endï¼‰        â”‚
â”‚  æ•´ä½“ç”¨æˆ·ä½“éªŒå¦‚ä½•ï¼Ÿ                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ä¸€ã€æ£€ç´¢è¯„ä¼°æŒ‡æ ‡

### æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡ | å®šä¹‰ | è®¡ç®—å…¬å¼ | ç†æƒ³å€¼ |
|------|------|----------|--------|
| **Precision@K** | å‰Kä¸ªç»“æœä¸­æ­£ç¡®çš„æ¯”ä¾‹ | æ­£ç¡®æ–‡æ¡£æ•° / K | è¶Šé«˜è¶Šå¥½ |
| **Recall@K** | å‰Kä¸ªç»“æœè¦†ç›–äº†å¤šå°‘æ­£ç¡®æ–‡æ¡£ | æ£€ç´¢åˆ°çš„æ­£ç¡®æ–‡æ¡£æ•° / æ€»æ­£ç¡®æ–‡æ¡£æ•° | è¶Šé«˜è¶Šå¥½ |
| **MRR** | ç¬¬ä¸€ä¸ªæ­£ç¡®ç»“æœçš„æ’åå€’æ•° | 1 / ç¬¬ä¸€ä¸ªæ­£ç¡®ç»“æœçš„ä½ç½® | è¶Šé«˜è¶Šå¥½ |
| **NDCG@K** | å½’ä¸€åŒ–æŠ˜æŸç´¯è®¡å¢ç›Š | è€ƒè™‘æ’åºè´¨é‡çš„ç»¼åˆæŒ‡æ ‡ | è¶Šé«˜è¶Šå¥½ |

### è¯„ä¼°ç¤ºä¾‹

```python
def evaluate_retrieval(query, retrieved_docs, ground_truth_docs, k=5):
    """
    è¯„ä¼°æ£€ç´¢æ•ˆæœ
    
    Args:
        query: æŸ¥è¯¢é—®é¢˜
        retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        ground_truth_docs: çœŸå®ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        k: è¯„ä¼°å‰kä¸ªç»“æœ
    """
    # å–å‰kä¸ªç»“æœ
    top_k_docs = retrieved_docs[:k]
    
    # Precision@K
    relevant_retrieved = [doc for doc in top_k_docs if doc in ground_truth_docs]
    precision_at_k = len(relevant_retrieved) / k
    
    # Recall@K
    recall_at_k = len(relevant_retrieved) / len(ground_truth_docs)
    
    # MRR
    for i, doc in enumerate(top_k_docs, 1):
        if doc in ground_truth_docs:
            mrr = 1 / i
            break
    else:
        mrr = 0
    
    return {
        "precision@k": precision_at_k,
        "recall@k": recall_at_k,
        "mrr": mrr
    }
```

### æ£€ç´¢è¯„ä¼°æµç¨‹

```python
# 1. å‡†å¤‡æµ‹è¯•é›†
test_queries = [
    {
        "query": "ä»€ä¹ˆæ˜¯RAGï¼Ÿ",
        "relevant_docs": ["doc_1", "doc_3", "doc_7"]
    },
    # ... æ›´å¤šæµ‹è¯•ç”¨ä¾‹
]

# 2. æ‰¹é‡è¯„ä¼°
results = []
for test_case in test_queries:
    retrieved = retriever.get_relevant_documents(test_case["query"])
    metrics = evaluate_retrieval(
        test_case["query"],
        retrieved,
        test_case["relevant_docs"]
    )
    results.append(metrics)

# 3. è®¡ç®—å¹³å‡æŒ‡æ ‡
avg_precision = np.mean([r["precision@k"] for r in results])
avg_recall = np.mean([r["recall@k"] for r in results])
print(f"å¹³å‡Precision@5: {avg_precision:.3f}")
print(f"å¹³å‡Recall@5: {avg_recall:.3f}")
```

## äºŒã€ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡

### ç­”æ¡ˆè´¨é‡ç»´åº¦

| ç»´åº¦ | è¯´æ˜ | è¯„ä¼°æ–¹æ³• |
|------|------|----------|
| **å‡†ç¡®æ€§** | ç­”æ¡ˆæ˜¯å¦æ­£ç¡® | äººå·¥æ ‡æ³¨ / LLMè¯„åˆ† |
| **å®Œæ•´æ€§** | æ˜¯å¦å›ç­”äº†æ‰€æœ‰æ–¹é¢ | è¦ç‚¹è¦†ç›–ç‡ |
| **ç›¸å…³æ€§** | æ˜¯å¦åˆ‡é¢˜ | è¯­ä¹‰ç›¸ä¼¼åº¦ |
| **å¿ å®æ€§** | æ˜¯å¦åŸºäºå‚è€ƒèµ„æ–™ | äº‹å®ä¸€è‡´æ€§æ£€æŸ¥ |
| **æµç•…æ€§** | è¯­è¨€æ˜¯å¦è‡ªç„¶ | å›°æƒ‘åº¦ / äººå·¥è¯„åˆ† |

### LLM-as-Judge è¯„ä¼°

ä½¿ç”¨å¼ºå¤§çš„LLMï¼ˆå¦‚GPT-4ï¼‰è¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼š

```python
def llm_judge_answer(question, answer, reference_docs):
    """ä½¿ç”¨LLMè¯„ä¼°ç­”æ¡ˆè´¨é‡"""
    
    judge_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„å®¡ä¸“å®¶ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆçš„è´¨é‡ã€‚

    ã€é—®é¢˜ã€‘
    {question}

    ã€å‚è€ƒèµ„æ–™ã€‘
    {reference_docs}

    ã€ç”Ÿæˆçš„ç­”æ¡ˆã€‘
    {answer}

    è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰ï¼š

    1. å‡†ç¡®æ€§ï¼ˆAccuracyï¼‰ï¼šç­”æ¡ˆæ˜¯å¦æ­£ç¡®
       - 5åˆ†ï¼šå®Œå…¨å‡†ç¡®
       - 3åˆ†ï¼šåŸºæœ¬å‡†ç¡®ä½†æœ‰å°é”™è¯¯
       - 1åˆ†ï¼šé”™è¯¯æˆ–è¯¯å¯¼

    2. å®Œæ•´æ€§ï¼ˆCompletenessï¼‰ï¼šæ˜¯å¦å…¨é¢å›ç­”
       - 5åˆ†ï¼šå›ç­”äº†æ‰€æœ‰è¦ç‚¹
       - 3åˆ†ï¼šå›ç­”äº†ä¸»è¦è¦ç‚¹
       - 1åˆ†ï¼šé—æ¼é‡è¦ä¿¡æ¯

    3. å¿ å®æ€§ï¼ˆFaithfulnessï¼‰ï¼šæ˜¯å¦åŸºäºå‚è€ƒèµ„æ–™
       - 5åˆ†ï¼šå®Œå…¨åŸºäºå‚è€ƒèµ„æ–™
       - 3åˆ†ï¼šå¤§éƒ¨åˆ†åŸºäºï¼Œæœ‰å°‘é‡æ¨æµ‹
       - 1åˆ†ï¼šåŒ…å«å‚è€ƒèµ„æ–™å¤–çš„å†…å®¹

    4. ç›¸å…³æ€§ï¼ˆRelevanceï¼‰ï¼šæ˜¯å¦åˆ‡é¢˜
       - 5åˆ†ï¼šå®Œå…¨åˆ‡é¢˜
       - 3åˆ†ï¼šåŸºæœ¬ç›¸å…³ä½†æœ‰åç¦»
       - 1åˆ†ï¼šç­”éæ‰€é—®

    è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
    å‡†ç¡®æ€§ï¼š[åˆ†æ•°] - [ç†ç”±]
    å®Œæ•´æ€§ï¼š[åˆ†æ•°] - [ç†ç”±]
    å¿ å®æ€§ï¼š[åˆ†æ•°] - [ç†ç”±]
    ç›¸å…³æ€§ï¼š[åˆ†æ•°] - [ç†ç”±]
    æ€»åˆ†ï¼š[å¹³å‡åˆ†]
    """
    
    evaluation = llm.invoke(judge_prompt)
    return parse_evaluation(evaluation)
```

### RAGASè¯„ä¼°æ¡†æ¶

[RAGAS](https://github.com/explodinggradients/ragas) æ˜¯ä¸“é—¨ä¸ºRAGè®¾è®¡çš„è¯„ä¼°æ¡†æ¶ï¼š

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,         # å¿ å®æ€§
    answer_relevancy,     # ç­”æ¡ˆç›¸å…³æ€§
    context_relevancy,    # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
    context_recall,       # ä¸Šä¸‹æ–‡å¬å›ç‡
)

# å‡†å¤‡è¯„ä¼°æ•°æ®
eval_data = {
    "question": ["ä»€ä¹ˆæ˜¯RAGï¼Ÿ", ...],
    "answer": ["RAGæ˜¯...", ...],
    "contexts": [[doc1, doc2], ...],
    "ground_truths": [["æ ‡å‡†ç­”æ¡ˆ1"], ...]
}

# è¿è¡Œè¯„ä¼°
results = evaluate(
    dataset=eval_data,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_relevancy,
        context_recall,
    ]
)

print(results)
```

## ä¸‰ã€ç«¯åˆ°ç«¯è¯„ä¼°

### ç”¨æˆ·ä½“éªŒæŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | è·å–æ–¹å¼ |
|------|------|----------|
| **å“åº”æ—¶é—´** | ä»æé—®åˆ°å¾—åˆ°ç­”æ¡ˆçš„æ—¶é—´ | ç³»ç»Ÿæ—¥å¿— |
| **ç”¨æˆ·æ»¡æ„åº¦** | ç”¨æˆ·å¯¹ç­”æ¡ˆçš„æ»¡æ„ç¨‹åº¦ | ğŸ‘ğŸ‘åé¦ˆ |
| **é—®é¢˜è§£å†³ç‡** | ä¸€æ¬¡æ€§è§£å†³é—®é¢˜çš„æ¯”ä¾‹ | æ˜¯å¦ç»§ç»­è¿½é—® |
| **ä¼šè¯è½®æ•°** | å¹³å‡å¯¹è¯è½®æ•° | å¯¹è¯æ—¥å¿— |

### A/Bæµ‹è¯•

å¯¹æ¯”ä¸åŒRAGç­–ç•¥çš„æ•ˆæœï¼š

```python
# ç‰ˆæœ¬Aï¼šåŸºç¡€å‘é‡æ£€ç´¢
retriever_a = VectorRetriever(embedding_model="text-embedding-3-small")

# ç‰ˆæœ¬Bï¼šå‘é‡+Rerank
retriever_b = VectorRetriever(embedding_model="text-embedding-3-small")
retriever_b = add_reranker(retriever_b)

# éšæœºåˆ†é…ç”¨æˆ·
def get_retriever(user_id):
    if hash(user_id) % 2 == 0:
        return retriever_a, "A"
    else:
        return retriever_b, "B"

# æ”¶é›†æŒ‡æ ‡
metrics_a = []
metrics_b = []

# åˆ†æç»“æœ
print(f"ç‰ˆæœ¬A å¹³å‡æ»¡æ„åº¦: {np.mean(metrics_a)}")
print(f"ç‰ˆæœ¬B å¹³å‡æ»¡æ„åº¦: {np.mean(metrics_b)}")
```

## å››ã€è¯„ä¼°æ•°æ®é›†æ„å»º

### æ–¹æ³•1ï¼šäººå·¥æ ‡æ³¨

```python
# æ ‡æ³¨æ ¼å¼
test_cases = [
    {
        "query": "å…¬å¸çš„å¹´å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
        "relevant_docs": ["doc_123", "doc_456"],  # ç›¸å…³æ–‡æ¡£ID
        "ideal_answer": "æ ¹æ®å…¬å¸æ”¿ç­–ï¼Œå‘˜å·¥å…¥èŒæ»¡ä¸€å¹´äº«æœ‰5å¤©å¹´å‡...",  # ç†æƒ³ç­”æ¡ˆ
        "must_include": ["5å¤©", "å…¥èŒæ»¡ä¸€å¹´"],  # å¿…é¡»åŒ…å«çš„å…³é”®ä¿¡æ¯
        "must_not_include": ["ç—…å‡", "äº‹å‡"]  # ä¸åº”åŒ…å«çš„å†…å®¹
    }
]
```

### æ–¹æ³•2ï¼šä»æ—¥å¿—ä¸­æå–

```python
# ä»ç”¨æˆ·åé¦ˆä¸­ç­›é€‰å¥½åæ¡ˆä¾‹
good_cases = filter_logs(
    feedback="positive",
    min_confidence=0.8
)

bad_cases = filter_logs(
    feedback="negative"
)

# åˆ†æå·®å¼‚ï¼Œæ‰¾åˆ°ä¼˜åŒ–æ–¹å‘
```

### æ–¹æ³•3ï¼šåˆæˆæ•°æ®

```python
# ä½¿ç”¨LLMç”Ÿæˆæµ‹è¯•é—®é¢˜
def generate_test_questions(document):
    prompt = f"""
    åŸºäºä»¥ä¸‹æ–‡æ¡£ï¼Œç”Ÿæˆ5ä¸ªä¸åŒéš¾åº¦çš„æµ‹è¯•é—®é¢˜ï¼š

    æ–‡æ¡£ï¼š
    {document}

    è¦æ±‚ï¼š
    1. åŒ…å«ç®€å•äº‹å®æ€§é—®é¢˜
    2. åŒ…å«éœ€è¦æ¨ç†çš„é—®é¢˜
    3. åŒ…å«éœ€è¦ç»¼åˆå¤šå¤„ä¿¡æ¯çš„é—®é¢˜

    æ ¼å¼ï¼š
    é—®é¢˜1ï¼š[é—®é¢˜å†…å®¹]
    ç­”æ¡ˆ1ï¼š[å‚è€ƒç­”æ¡ˆ]
    """
    
    return llm.invoke(prompt)
```

## äº”ã€æŒç»­ä¼˜åŒ–æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. æ”¶é›†æ•°æ®  â”‚ â†’ ç”¨æˆ·æŸ¥è¯¢æ—¥å¿—ã€åé¦ˆ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. è¿è¡Œè¯„ä¼°  â”‚ â†’ æ‰¹é‡è¯„ä¼°ç°æœ‰ç³»ç»Ÿ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. åˆ†æç“¶é¢ˆ  â”‚ â†’ æ£€ç´¢å·®ï¼Ÿç”Ÿæˆå·®ï¼Ÿ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. é’ˆå¯¹ä¼˜åŒ–  â”‚ â†’ è°ƒæ•´Promptã€æ¨¡å‹ã€å‚æ•°
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. A/Bæµ‹è¯•   â”‚ â†’ å¯¹æ¯”æ–°æ—§ç‰ˆæœ¬
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. ä¸Šçº¿è¿­ä»£  â”‚ â†’ æŒç»­æ”¹è¿›
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å…­ã€è¯„ä¼°å·¥å…·æ¨è

### 1. RAGAS
- ä¸“ä¸ºRAGè®¾è®¡
- æä¾›å¤šç§è¯„ä¼°æŒ‡æ ‡
- æ”¯æŒLangChainé›†æˆ
- é¡¹ç›®åœ°å€ï¼šhttps://github.com/explodinggradients/ragas

### 2. TruLens
- å¯è§†åŒ–è¯„ä¼°é¢æ¿
- å®æ—¶ç›‘æ§RAGæ€§èƒ½
- æä¾›è°ƒè¯•å·¥å…·
- é¡¹ç›®åœ°å€ï¼šhttps://github.com/truera/trulens

### 3. Phoenix (by Arize AI)
- å¼€æºè¯„ä¼°å¹³å°
- LLMå¯è§‚æµ‹æ€§
- æ”¯æŒè¿½è¸ªå’Œåˆ†æ
- é¡¹ç›®åœ°å€ï¼šhttps://github.com/Arize-ai/phoenix

### 4. LangSmith
- LangChainå®˜æ–¹å·¥å…·
- å®Œæ•´çš„è°ƒè¯•å’Œè¯„ä¼°åŠŸèƒ½
- æ”¯æŒå›¢é˜Ÿåä½œ
- ç½‘ç«™ï¼šhttps://smith.langchain.com/

## ä¸ƒã€å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ï¼šä¼˜åŒ–å®¢æœçŸ¥è¯†åº“

```python
# åˆå§‹è¯„ä¼°
baseline_metrics = evaluate_system(test_set)
# ç»“æœï¼šå‡†ç¡®ç‡ 65%ï¼Œç”¨æˆ·æ»¡æ„åº¦ 3.2/5

# é—®é¢˜åˆ†æ
# 1. æ£€ç´¢é—®é¢˜ï¼šRecall@5 åªæœ‰ 40%
# 2. ç”Ÿæˆé—®é¢˜ï¼šç­”æ¡ˆä¸å¤Ÿå‹å¥½ï¼Œè¿‡äºå®˜æ–¹

# ä¼˜åŒ–1ï¼šæ”¹è¿›æ£€ç´¢ï¼ˆåŠ å…¥Rerankï¼‰
retriever = add_reranker(retriever)
new_metrics_1 = evaluate_system(test_set)
# ç»“æœï¼šå‡†ç¡®ç‡ 75% â†‘ï¼ŒRecall@5 65% â†‘

# ä¼˜åŒ–2ï¼šæ”¹è¿›Promptï¼ˆè°ƒæ•´è¯­æ°”ï¼‰
prompt = update_prompt_with_friendly_tone(prompt)
new_metrics_2 = evaluate_system(test_set)
# ç»“æœï¼šç”¨æˆ·æ»¡æ„åº¦ 4.1/5 â†‘

# A/Bæµ‹è¯•éªŒè¯
ab_test_results = run_ab_test(baseline, optimized, days=7)
# æ–°ç‰ˆæœ¬è·èƒœï¼Œå…¨é‡ä¸Šçº¿
```

## å¸¸è§é—®é¢˜

### Q1: è¯„ä¼°æ•°æ®é›†éœ€è¦å¤šå¤§ï¼Ÿ
**A**: 
- å°è§„æ¨¡ï¼š50-100æ¡ï¼ˆå¿«é€ŸéªŒè¯ï¼‰
- ä¸­ç­‰è§„æ¨¡ï¼š500-1000æ¡ï¼ˆå¯é è¯„ä¼°ï¼‰
- å¤§è§„æ¨¡ï¼š5000+æ¡ï¼ˆå……åˆ†è¯„ä¼°ï¼‰

### Q2: äººå·¥è¯„ä¼° vs è‡ªåŠ¨è¯„ä¼°ï¼Ÿ
**A**: 
- å¼€å‘é˜¶æ®µï¼šè‡ªåŠ¨è¯„ä¼°ä¸ºä¸»ï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰
- ä¸Šçº¿å‰ï¼šäººå·¥è¯„ä¼°å…³é”®æ¡ˆä¾‹
- æŒç»­ç›‘æ§ï¼šè‡ªåŠ¨è¯„ä¼° + é‡‡æ ·äººå·¥å®¡æ ¸

### Q3: å¦‚ä½•å¤„ç†ä¸»è§‚æ€§é—®é¢˜ï¼Ÿ
**A**: 
- åˆ¶å®šæ˜ç¡®çš„è¯„åˆ†æ ‡å‡†
- å¤šäººæ ‡æ³¨å–å¹³å‡
- ä½¿ç”¨LLMè¾…åŠ©è¯„ä¼°

## å®è·µç»ƒä¹ 

### ğŸ““ rag_evaluation.ipynbï¼ˆå¼€å‘ä¸­ï¼‰

è®¡åˆ’åŒ…å«ï¼š

1. **æ£€ç´¢è¯„ä¼°å®æˆ˜**
   - æ„å»ºæµ‹è¯•é›†
   - è®¡ç®—å„é¡¹æŒ‡æ ‡
   - å¯è§†åŒ–ç»“æœ

2. **ç”Ÿæˆè¯„ä¼°å®æˆ˜**
   - LLM-as-Judgeå®ç°
   - RAGASæ¡†æ¶ä½¿ç”¨

3. **A/Bæµ‹è¯•å®æˆ˜**
   - å¯¹æ¯”ä¸åŒç­–ç•¥
   - ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

4. **ä¼˜åŒ–æ¡ˆä¾‹**
   - çœŸå®é—®é¢˜åˆ†æ
   - è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹

## ä¸‹ä¸€æ­¥

ğŸ‰ æ­å–œå®ŒæˆRAGç³»ç»Ÿçš„å®Œæ•´å­¦ä¹ ï¼æ¥ä¸‹æ¥å¯ä»¥ï¼š

â¡ï¸ æ¢ç´¢ [æœªæ¥æŠ€æœ¯æ–¹å‘](../future_practice/Readme.md)
â¡ï¸ é˜…è¯» [ç›¸å…³è®ºæ–‡](../paper_learning/)
â¡ï¸ éƒ¨ç½²è‡ªå·±çš„RAGåº”ç”¨

---

ğŸ’¡ **æç¤º**ï¼šè¯„ä¼°æ˜¯æŒç»­æ”¹è¿›çš„åŸºç¡€ã€‚å»ºè®®å»ºç«‹è¯„ä¼°æ•°æ®é›†ï¼Œæ¯æ¬¡ä¼˜åŒ–éƒ½å¯¹æ¯”æŒ‡æ ‡å˜åŒ–ï¼Œç¡®ä¿æ”¹è¿›æœ‰æ•ˆï¼

