{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过将数据进行预处理之后，我们就需要来构建我们的数据检索系统\n",
    "步骤：\n",
    "1. 选择检索方式（关键词检索、向量检索、图检索、混合检索）\n",
    "2. 选择检索数据库（Faiss、Annoy、BM25）\n",
    "3. 选择模型（embeding模型+rerank模型+llm模型）\n",
    "4. 调整检索参数\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么场景下使用哪种检索方式？\n",
    "1. 关键词检索：搜索之前已经能够准确知道检索内容，且更适合查询数字id，例如查询文件中身份证是xxx的内容\n",
    "2. 向量检索：搜索之前只知道大概是什么意思，但是不了解具体官方叫法，例如：有时候需要找盖章流程，但是不知道具体叫法，就可以通过向量检索最终找到用印申请流程\n",
    "3. 图检索：在有大量复杂关系的长文本中搜索或者需要概括各种关系流程，例如：搜索整本小说的任务故事线，或者搜索整本小说中的人物关系（这种检索对于本质上是以部分内容回答的向量检索来说，效果会更好，更全面）\n",
    "4. 混合检索：如果以上三种需求都有，则可以考虑混合检索多路找回后进行结果融合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键词检索\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 建议使用Elasticsearch，langchain自带的bm25效果很差，也不是传统意义的关键词检索，感兴趣的可以查询一下原理\n",
    "docker pull docker.elastic.co/elasticsearch/elasticsearch:8.15.3\n",
    "docker network create elastic\n",
    "## 可以把端口和密码换一下\n",
    "docker run -d --name es01 --net elastic -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=true\" -e \"ELASTIC_PASSWORD=your_password\" docker.elastic.co/elasticsearch/elasticsearch:8.15.3\n",
    "## 测试一下是否可以连接\n",
    "curl -X GET \"http://localhost:9200\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.bfsu.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: elasticsearch in /data/xieyu/media_monitor/venv/lib/python3.10/site-packages (8.16.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in /data/xieyu/media_monitor/venv/lib/python3.10/site-packages (from elasticsearch) (8.15.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /data/xieyu/media_monitor/venv/lib/python3.10/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.2.3)\n",
      "Requirement already satisfied: certifi in /data/xieyu/media_monitor/venv/lib/python3.10/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch pandas python-docx --quiet --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 如果你想要深入学习Elasticsearch，可以参考以下代码和官方文档\n",
    " \n",
    " [参考代码][1] \n",
    " \n",
    " [官方文档][2]\n",
    "\n",
    "[1]: https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/\n",
    "[2]: https://elasticsearch-py.readthedocs.io/en/v8.16.0/interactive.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import urllib3\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 禁用不安全的 HTTPS 警告（仅用于测试，生产环境应该使用 HTTPS）\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# 设置日志级别以查看更多信息\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# 初始化 Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    hosts=['http://localhost:39200'],\n",
    "    basic_auth=('elastic', 'Xieyu120807'),\n",
    "    verify_certs=False,  # 仅用于测试，生产环境应该验证证书\n",
    "    request_timeout=30\n",
    ")\n",
    "index_name = \"rag_teach\"\n",
    "\n",
    "# ## 需要把数据转换为json格式\n",
    "# ### txt文件\n",
    "def index_txt_file(file_path, index_name):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    document = {\n",
    "        \"title\": file_path.split(\"/\")[-1],\n",
    "        \"content\": content\n",
    "    }\n",
    "    es.index(index=index_name, document=document)\n",
    "\n",
    "# ### csv文件\n",
    "# def index_csv_file(file_path, index_name):\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     actions = [\n",
    "#         {\n",
    "#             \"_index\": index_name,\n",
    "#             \"_source\": row.to_dict()\n",
    "#         }\n",
    "#         for _, row in df.iterrows()\n",
    "#     ]\n",
    "    \n",
    "#     success, _ = bulk(es, actions)\n",
    "#     print(f\"已成功索引 {success} 条记录\")\n",
    "\n",
    "# ### docx文件\n",
    "# def index_word_document(file_path, index_name):\n",
    "#     doc = Document(file_path)\n",
    "#     full_text = []\n",
    "#     for para in doc.paragraphs:\n",
    "#         full_text.append(para.text)\n",
    "    \n",
    "#     document = {\n",
    "#         \"title\": file_path.split(\"/\")[-1],\n",
    "#         \"content\": \"\\n\".join(full_text)\n",
    "#     }\n",
    "    \n",
    "#     es.index(index=index_name, document=document)\n",
    "#     print(f\"已索引Word文档: {file_path}\")\n",
    "\n",
    "\n",
    "# 导入数据\n",
    "index_txt_file(file_path=\"/data/xieyu/Teaching/RAG/RAG的道与术.txt\", index_name=\"rag_teach\")\n",
    "\n",
    "keyword = \"背景\"\n",
    " \n",
    "### 直接的关键词检索\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content\": keyword\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "results = es.search(index=index_name, body=query)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多种关键词查询方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入多维测试数据\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from datetime import datetime\n",
    "\n",
    "# 连接到Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:39200\", basic_auth=('elastic', 'Xieyu120807'), verify_certs=False)\n",
    "\n",
    "# 确保索引存在\n",
    "index_name = \"book_index\"\n",
    "if not es.indices.exists(index=index_name):\n",
    "    print(f\"索引 {index_name} 不存在，正在创建并导入示例数据。\")\n",
    "    \n",
    "    # 创建示例数据\n",
    "    sample_books = [\n",
    "        {\n",
    "            \"title\": \"Python编程指南\",\n",
    "            \"authors\": [\"John Smith\", \"Jane Doe\"],\n",
    "            \"publisher\": \"技术出版社\",\n",
    "            \"publish_date\": \"2023-01-15\",\n",
    "            \"summary\": \"这是一本全面的Python编程指南，适合初学者和中级程序员。\",\n",
    "            \"num_reviews\": 120\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"JavaScript高级教程\",\n",
    "            \"authors\": [\"Mike Johnson\"],\n",
    "            \"publisher\": \"Addison-Wesley\",\n",
    "            \"publish_date\": \"2022-11-30\",\n",
    "            \"summary\": \"深入探讨JavaScript的高级特性和最佳实践。\",\n",
    "            \"num_reviews\": 85\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"数据科学实战\",\n",
    "            \"authors\": [\"Sarah Lee\", \"Tom Brown\"],\n",
    "            \"publisher\": \"数据出版集团\",\n",
    "            \"publish_date\": \"2023-03-22\",\n",
    "            \"summary\": \"通过实际项目学习数据科学，包括Python和R语言的应用。\",\n",
    "            \"num_reviews\": 56\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 导入示例数据\n",
    "    for book in sample_books:\n",
    "        es.index(index=index_name, document=book)\n",
    "    \n",
    "    print(\"示例数据已成功导入。\")\n",
    "else:\n",
    "    print(f\"索引 {index_name} 已存在，准备进行查询示例。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:39200/book_index/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:39200/book_index/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:39200/book_index/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:39200/book_index/_search [status:200 duration:0.005s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词 'Python' 的简单匹配查询结果:\n",
      "说明：这种查询方式会在summary字段中搜索包含指定关键词的文档。\n",
      "\n",
      "ID: ufCfOZMBE5wK0lLCF5Q1\n",
      "出版日期: 2023-03-22\n",
      "标题: 数据科学实战\n",
      "摘要: 通过实际项目学习数据科学，包括Python和R语言的应用。\n",
      "出版社: 数据出版集团\n",
      "评论数: 56\n",
      "作者: Sarah Lee, Tom Brown\n",
      "相关度得分: 0.45153183\n",
      "\n",
      "ID: t_CfOZMBE5wK0lLCFpT2\n",
      "出版日期: 2023-01-15\n",
      "标题: Python编程指南\n",
      "摘要: 这是一本全面的Python编程指南，适合初学者和中级程序员。\n",
      "出版社: 技术出版社\n",
      "评论数: 120\n",
      "作者: John Smith, Jane Doe\n",
      "相关度得分: 0.44283003\n",
      "关键词 '数据科学' 的多字段查询结果:\n",
      "说明：这种查询方式会在summary和title字段中搜索包含指定关键词的文档。\n",
      "\n",
      "ID: ufCfOZMBE5wK0lLCF5Q1\n",
      "出版日期: 2023-03-22\n",
      "标题: 数据科学实战\n",
      "摘要: 通过实际项目学习数据科学，包括Python和R语言的应用。\n",
      "出版社: 数据出版集团\n",
      "评论数: 56\n",
      "作者: Sarah Lee, Tom Brown\n",
      "相关度得分: 3.7324529\n",
      "\n",
      "ID: t_CfOZMBE5wK0lLCFpT2\n",
      "出版日期: 2023-01-15\n",
      "标题: Python编程指南\n",
      "摘要: 这是一本全面的Python编程指南，适合初学者和中级程序员。\n",
      "出版社: 技术出版社\n",
      "评论数: 120\n",
      "作者: John Smith, Jane Doe\n",
      "相关度得分: 0.44283003\n",
      "关键词 '编程' 的带权重多字段查询结果:\n",
      "说明：这种查询方式会在summary和title字段中搜索，但title字段的权重是summary的3倍。\n",
      "\n",
      "ID: t_CfOZMBE5wK0lLCFpT2\n",
      "出版日期: 2023-01-15\n",
      "标题: Python编程指南\n",
      "摘要: 这是一本全面的Python编程指南，适合初学者和中级程序员。\n",
      "出版社: 技术出版社\n",
      "评论数: 120\n",
      "作者: John Smith, Jane Doe\n",
      "相关度得分: 4.4667044\n",
      "\n",
      "ID: uPCfOZMBE5wK0lLCF5Qr\n",
      "出版日期: 2022-11-30\n",
      "标题: JavaScript高级教程\n",
      "摘要: 深入探讨JavaScript的高级特性和最佳实践。\n",
      "出版社: Addison-Wesley\n",
      "评论数: 85\n",
      "作者: Mike Johnson\n",
      "相关度得分: 1.4470083\n",
      "出版社 '技术出版社' 的精确匹配查询结果:\n",
      "说明：这种查询方式要求publisher字段完全匹配指定的值，不进行分词。\n",
      "\n",
      "ID: t_CfOZMBE5wK0lLCFpT2\n",
      "出版日期: 2023-01-15\n",
      "标题: Python编程指南\n",
      "摘要: 这是一本全面的Python编程指南，适合初学者和中级程序员。\n",
      "出版社: 技术出版社\n",
      "评论数: 120\n",
      "作者: John Smith, Jane Doe\n",
      "相关度得分: 0.9808291\n",
      "\n",
      "简单关键词查询解释:\n",
      "这个查询会在summary字段中搜索包含'Python'的文档。\n",
      "预期会匹配到'Python编程指南'这本书。\n",
      "--------------------------------\n",
      "\n",
      "多字段查询解释:\n",
      "这个查询会在summary和title字段中搜索包含'数据科学'的文档。\n",
      "预期会匹配到'数据科学实战'这本书。\n",
      "--------------------------------\n",
      "\n",
      "带权重的多字段查询解释:\n",
      "这个查询会在summary和title字段中搜索包含'编程'的文档，但title字段的权重更高。\n",
      "预期可能会匹配到'Python编程指南'，因为'编程'在title中出现。\n",
      "--------------------------------\n",
      "\n",
      "精确匹配查询解释:\n",
      "这个查询要求publisher字段完全匹配'技术出版社'。\n",
      "预期会匹配到'Python编程指南'这本书。\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "## 多种查询方式示例\n",
    "# 定义美化输出函数\n",
    "def pretty_response(response):\n",
    "    if len(response[\"hits\"][\"hits\"]) == 0:\n",
    "        print(\"您的搜索没有返回结果。\")\n",
    "    else:\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            id = hit[\"_id\"]\n",
    "            publication_date = hit[\"_source\"][\"publish_date\"]\n",
    "            score = hit[\"_score\"]\n",
    "            title = hit[\"_source\"][\"title\"]\n",
    "            summary = hit[\"_source\"][\"summary\"]\n",
    "            publisher = hit[\"_source\"][\"publisher\"]\n",
    "            num_reviews = hit[\"_source\"][\"num_reviews\"]\n",
    "            authors = \", \".join(hit[\"_source\"][\"authors\"])\n",
    "            pretty_output = f\"\\nID: {id}\\n出版日期: {publication_date}\\n标题: {title}\\n摘要: {summary}\\n出版社: {publisher}\\n评论数: {num_reviews}\\n作者: {authors}\\n相关度得分: {score}\"\n",
    "            print(pretty_output)\n",
    "\n",
    "# 1. 简单的关键词匹配查询\n",
    "def simple_keyword_query(keyword):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"summary\": keyword\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    results = es.search(index=index_name, body=query)\n",
    "    print(f\"关键词 '{keyword}' 的简单匹配查询结果:\")\n",
    "    print(\"说明：这种查询方式会在summary字段中搜索包含指定关键词的文档。\")\n",
    "    pretty_response(results)\n",
    "\n",
    "# 2. 多字段查询\n",
    "def multi_field_query(keyword):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": keyword,\n",
    "                \"fields\": [\"summary\", \"title\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    results = es.search(index=index_name, body=query)\n",
    "    print(f\"关键词 '{keyword}' 的多字段查询结果:\")\n",
    "    print(\"说明：这种查询方式会在summary和title字段中搜索包含指定关键词的文档。\")\n",
    "    pretty_response(results)\n",
    "\n",
    "# 3. 带权重的多字段查询\n",
    "def weighted_multi_field_query(keyword):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": keyword,\n",
    "                \"fields\": [\"summary\", \"title^3\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    results = es.search(index=index_name, body=query)\n",
    "    print(f\"关键词 '{keyword}' 的带权重多字段查询结果:\")\n",
    "    print(\"说明：这种查询方式会在summary和title字段中搜索，但title字段的权重是summary的3倍。\")\n",
    "    pretty_response(results)\n",
    "\n",
    "# 4. 精确匹配查询\n",
    "def term_query(publisher):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"term\": {\n",
    "                \"publisher.keyword\": publisher\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    results = es.search(index=index_name, body=query)\n",
    "    print(f\"出版社 '{publisher}' 的精确匹配查询结果:\")\n",
    "    print(\"说明：这种查询方式要求publisher字段完全匹配指定的值，不进行分词。\")\n",
    "    pretty_response(results)\n",
    "\n",
    "# 执行查询示例\n",
    "# 执行查询示例\n",
    "simple_keyword_query(\"Python\")\n",
    "multi_field_query(\"数据科学\")\n",
    "weighted_multi_field_query(\"编程\")\n",
    "term_query(\"技术出版社\")\n",
    "\n",
    "# 解释每个查询的结果\n",
    "print(\"\\n简单关键词查询解释:\")\n",
    "print(\"这个查询会在summary字段中搜索包含'Python'的文档。\")\n",
    "print(\"预期会匹配到'Python编程指南'这本书。\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"\\n多字段查询解释:\")\n",
    "print(\"这个查询会在summary和title字段中搜索包含'数据科学'的文档。\")\n",
    "print(\"预期会匹配到'数据科学实战'这本书。\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"\\n带权重的多字段查询解释:\")\n",
    "print(\"这个查询会在summary和title字段中搜索包含'编程'的文档，但title字段的权重更高。\")\n",
    "print(\"预期可能会匹配到'Python编程指南'，因为'编程'在title中出现。\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"\\n精确匹配查询解释:\")\n",
    "print(\"这个查询要求publisher字段完全匹配'技术出版社'。\")\n",
    "print(\"预期会匹配到'Python编程指南'这本书。\")\n",
    "print(\"--------------------------------\")\n",
    "# 注意：实际结果可能会因为Elasticsearch的评分机制而略有不同\n",
    "\n",
    "# 注意：这些查询示例基于我们创建的示例数据。\n",
    "# 在实际应用中，您可能需要根据实际的数据结构和需求调整查询。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "说明：向量检索主要分为四个步骤：\n",
    "1. 把文本内容分块（如果非文本内容请参考上一个章节）\n",
    "2. 把分块后的文本内容转换为向量 \n",
    "3. 把向量保存到数据库中（Faiss、Annoy、Chroma）\n",
    "4. 检索时，把查询内容转换为向量，在数据库中查找最相似的向量，返回结果\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/data/xieyu/Teaching/RAG/imgs/embeding_process.png\" alt=\"向量检索流程图\" width=\"30%\" style=\"display: block; margin: auto;\">\n",
    "<p style=\"text-align: center;\">上图展示了向量检索的基本流程，包括文本分块、向量转换、向量存储和相似度检索等步骤。</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/data/xieyu/Teaching/RAG/RAG的道与术.txt'}, page_content='RAG的道与术\\n目录\\n\\n目标读者\\n想要开始了解并学习 RAG技术的人\\n想要利用RAG实现智能知识问答的人\\nRAG是什么？\\n定义\\nRAG是一种能让大量知识 “活” 起来，人与知识无障碍对话的技术\\n背景\\nAI大模型已经能够和用户流畅交流，解读知识，总结文章，但由于训练他们的知识是互联网过往的公开内容，当我们有非公开的知识想让AI帮忙解读，总结时就涉及到他们的知识盲区，所以RAG技术就是让他们能够了解到我们的非公开知识，更加精准地回答我们的问题\\n场景\\n企业知识库\\n客服机器人\\n产品自动推荐\\nRAG主流方案\\n核心步骤\\n收集数据\\n导入数据\\n问题检索\\n结果评估\\n迭代优化\\n用dify进行演示\\n主流应用\\nlangchain\\n上手方便，装包随时可以使用\\ndbgpt\\n比较通用，api使用方便，原生支持graphRAG\\nragflow\\n文档处理最方便\\nQanything\\n检索到的重排序效果最好\\nfastgpt\\n接入模块易操作\\ndify\\n结合工作流更方便\\n制作对应的表格\\nRAG实践问题\\n核心目标\\n提高AI回复知识库相关问题的准确性\\n实践痛点\\n提问的问题稍微跟原文换一种说法就回答不上来\\n根据自己的理解问问题\\n根据自己的记忆关键词问问题\\n查询到的内容都不是自己要的内容\\n图片表格的知识信息无法查询\\n对长文本提问总结性问题总是回答不全\\n对于查询不到的问题进行编造强行回答\\n知识库特别大，难以完全测试来保证准确度\\n优化步骤\\n知道放什么知识？\\n知道怎么放知识？\\n知道怎么查知识？\\n知道查的对不对？\\n实践拆解\\n知道放什么知识？\\n范围\\n尽量只放入跟后续提问相关的内容，无效的内容越多，查询准确率越低\\n质量\\n内容要求要准确严谨，互相矛盾，避免大模型产生混乱\\n类型\\n本质上大模型只能够回答文本的内容，所以导入的文档应该尽量转化成普通文本模式\\n知识到怎么放知识\\n知识类型\\n纯文本导入\\n带图片文本导入\\n带表格文本导入\\n导入数据库\\nrelation-db\\nstr\\ngraph-db\\nnode\\nvector-db\\n转化成向量（世界通用语言）\\n知道怎么查知识？\\nquery_write\\n大模型改写\\nsearch_engine\\nvector\\nembeding\\nrerank\\nkeywords\\ngraphRAG\\n好问题+好引擎= 好答案\\n知道查的对不对？\\n检索内容相关度\\n大模型的理解表达正确度')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PS： 由于向量检索需要把文本内容转换为向量存储到向量库进行存储后方便检索，但是向量库检索有维度的限制，维度越大，记录越详细，\n",
    "但一般模型的维度都是有限制的，然后如果全部文本转换为向量，会导致内容压缩严重，导致后续检索不准\n",
    "所以需要对文本进行分块，然后对每块文本转换为向量后存储到向量库中，检索时，把查询内容转换为向量后，在向量库中查找最相似的向量，然后返回结果\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 加载文本\n",
    "loader = TextLoader(\"/data/xieyu/Teaching/RAG/RAG的道与术.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 创建文本分割器\n",
    "\"\"\"\n",
    "参数讲解：\n",
    "chunk_size：每块文本的大小\n",
    "chunk_overlap：每块文本之间的重叠部分（如果一个章节的内容太长，按照chunk_size分块后，可能会让原本的意思断开，所以设置chunk_overlap，则会把保留前后的文本，来保证文本的完整性）\n",
    "length_function：计算文本长度的函数，默认是len，即字符串长度\n",
    "\"\"\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# 分割文本\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "print(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量导入 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PS：向量导入首先要选择一个embeding模型，然后对文本分块转换为向量后，保存到向量库中\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 如何选择embeding模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你是第一次使用RAG，那非常建议你直接查看MTEB选择排名最好的模型，然后直接使用，但是要根据自己的预料的语言进行选择，中文就选zn的模型，英文就选en的模型\n",
    "<img src=\"/data/xieyu/Teaching/RAG/imgs/mteb_benchmark.png\" alt=\"MTEB排行榜\" width=\"30%\" style=\"display: block; margin: auto;\">\n",
    "<p style=\"text-align: center;\">上图展示了<a href=\"https://huggingface.co/spaces/mteb/leaderboard\" target=\"_blank\">MTEB排行榜</a>，可以参考选择模型</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你是一个老鸟，那考虑embeding模型主要考虑以下几个因素：\n",
    "1. <font color=\"white\">模型性能（准确率、召回率、F1值）</font>\n",
    "2. <font color=\"white\">模型速度</font>\n",
    "3. <font color=\"white\">支持向量维度大小</font>\n",
    "4. <font color=\"white\">模型大小</font>\n",
    "\n",
    "这些核心指标大部分可以从榜单中找到，但是具体还是应该<font color=\"red\">结合自己的数据集进行测试</font>，因为不同的数据集的效果差异很大，一定程度上需要对mteb祛魅，<font color=\"red\">基本选评分最高的不会错</font>，但是追求最好效果就需要测试，建议是用<font color=\"red\">xinference去一键部署，效率最高</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/data/xieyu/Teaching/RAG/imgs/xinference.png\" alt=\"xinference\" width=\"30%\" style=\"display: block; margin: auto;\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://github.com/xorbitsai/inference\" target=\"_blank\">xinference平台</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "使用方法：\n",
    "1. 使用本地下载的模型\n",
    "2. 使用xinference部署模型\n",
    "3. 使用openai接口类型的api\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "下载模型最好配置好cache_dir，这样模型会下载到cache_dir中，方便后续进行统一管理，如果保持默认值会放在一个临时目录中，比较难记得，后续多个项目要使用的时候不方便\n",
    "\"\"\"\n",
    "# from modelscope.hub.snapshot_download import snapshot_download\n",
    "# model_dir = snapshot_download('BAAI/bge-m3', cache_dir='/data/lilk/RAG/models/embeding', revision='v1.5')\n",
    "# 使用huggingface下载模型\n",
    "# from huggingface_hub import snapshot_download\n",
    "# snapshot_download(repo_id=\"ibrahimhamamci/CT-RATE\", repo_type=\"dataset\", cache_dir=\"/data/lilk/RAG/models/embeding\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai调用, 更加方便快捷，也不需要显卡，但很多国内的用户没有openai的账号，所以推荐使用硅基云的embeding模型，也支持openai的api格式的调用\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeding_model = OpenAIEmbeddings(model=\"BAAI/bge-m3\",api_key=\"sk-nmyyoncsmaagafmvjmbpyaxbeewtwqaiycitmhtomjzlwbsw\",base_url=\"https://api.siliconflow.cn/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain_huggingface --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lilk/RAG/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# 本地调用\n",
    "embeding_model_path = \"/data/lilk/RAG/models/embeding/bge-m3\"\n",
    "# 配置运行的参数\n",
    "embeding_model_kwargs = {\"device\": \"cuda\"}\n",
    "embeding_encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embeding_model = HuggingFaceEmbeddings(model_name = embeding_model_path,\n",
    "                                       model_kwargs=embeding_model_kwargs,\n",
    "                                       encode_kwargs=embeding_encode_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xinference调用\n",
    "from langchain_community.embeddings import XinferenceEmbeddings\n",
    "embeding_model = XinferenceEmbeddings(\n",
    "    server_url=\"http://0.0.0.0:19997\",  # Xinference服务的URL\n",
    "    model_uid=\"bge-m3\"  # 从步骤2中获取的模型UID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 如何选择一个向量库？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 向量数据库     | 特点                                                         |\n",
    "|--------------|--------------------------------------------------------------|\n",
    "| Milvus       | - 开源向量数据库<br>- 适合大规模向量数据存储和检索<br>- 支持多种ANN算法 |\n",
    "| Faiss        | - 由Meta开发的高效相似性搜索和密集向量聚类库<br>- 支持大规模向量集搜索 |\n",
    "| Weaviate     | - 开源向量数据库<br>- 支持水平和垂直扩展<br>- 适合数十亿数据对象 |\n",
    "| Pinecone     | - 专为机器学习设计的向量数据库<br>- 快速、可扩展<br>- 支持实时更新 |\n",
    "| Qdrant       | - 矢量相似度搜索引擎和数据库<br>- 支持分布式部署和水平扩展         |\n",
    "| Chroma       | - 轻量级、易用的向量数据库<br>- 适合小型到中型数据集             |\n",
    "| Elasticsearch| - 分布式搜索和分析引擎<br>- 从7.10版本开始支持向量字段           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量数据库的推荐其实也是要依赖数据的特点，但是刚开始实现一个项目的时候，往往不太清楚数据的特点，所以**选择更容易调用的比选择更适合的更重要**，构建完整闭环后再回来优化也不迟，这里选择Faiss数据库进行演示（<font color=\"red\">封装得比较好，该有的功能也都有</font>）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/data/xieyu/Teaching/RAG/RAG的道与术.txt'}, page_content='RAG的道与术\\n目录\\n\\n目标读者\\n想要开始了解并学习 RAG技术的人\\n想要利用RAG实现智能知识问答的人\\nRAG是什么？\\n定义\\nRAG是一种能让大量知识 “活” 起来，人与知识无障碍对话的技术\\n背景\\nAI大模型已经能够和用户流畅交流，解读知识，总结文章，但由于训练他们的知识是互联网过往的公开内容，当我们有非公开的知识想让AI帮忙解读，总结时就涉及到他们的知识盲区，所以RAG技术就是让他们能够了解到我们的非公开知识，更加精准地回答我们的问题\\n场景\\n企业知识库\\n客服机器人\\n产品自动推荐\\nRAG主流方案\\n核心步骤\\n收集数据\\n导入数据\\n问题检索\\n结果评估\\n迭代优化\\n用dify进行演示\\n主流应用\\nlangchain\\n上手方便，装包随时可以使用\\ndbgpt\\n比较通用，api使用方便，原生支持graphRAG\\nragflow\\n文档处理最方便\\nQanything\\n检索到的重排序效果最好\\nfastgpt\\n接入模块易操作\\ndify\\n结合工作流更方便\\n制作对应的表格\\nRAG实践问题\\n核心目标\\n提高AI回复知识库相关问题的准确性\\n实践痛点\\n提问的问题稍微跟原文换一种说法就回答不上来\\n根据自己的理解问问题\\n根据自己的记忆关键词问问题\\n查询到的内容都不是自己要的内容\\n图片表格的知识信息无法查询\\n对长文本提问总结性问题总是回答不全\\n对于查询不到的问题进行编造强行回答\\n知识库特别大，难以完全测试来保证准确度\\n优化步骤\\n知道放什么知识？\\n知道怎么放知识？\\n知道怎么查知识？\\n知道查的对不对？\\n实践拆解\\n知道放什么知识？\\n范围\\n尽量只放入跟后续提问相关的内容，无效的内容越多，查询准确率越低\\n质量\\n内容要求要准确严谨，互相矛盾，避免大模型产生混乱\\n类型\\n本质上大模型只能够回答文本的内容，所以导入的文档应该尽量转化成普通文本模式\\n知识到怎么放知识\\n知识类型\\n纯文本导入\\n带图片文本导入\\n带表格文本导入\\n导入数据库\\nrelation-db\\nstr\\ngraph-db\\nnode\\nvector-db\\n转化成向量（世界通用语言）\\n知道怎么查知识？\\nquery_write\\n大模型改写\\nsearch_engine\\nvector\\nembeding\\nrerank\\nkeywords\\ngraphRAG\\n好问题+好引擎= 好答案\\n知道查的对不对？\\n检索内容相关度\\n大模型的理解表达正确度')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "#  用刚刚的embeding模型和分块后的文本转换的向量来初始化向量库\n",
    "\"\"\"\n",
    "参数解析：\n",
    "embedding：引用刚刚初始化的embeding模型\n",
    "documents：分块后的文本\n",
    "docstore：用来存储导入的分块：InMemoryDocstore()是用内存来存储，但是如果数据量太大，可以使用ElasticSearchDocStore，或者SQLDocStore\n",
    "```\n",
    "    from langchain.docstore import SQLDocStore\n",
    "    docstore = SQLDocStore(\n",
    "        db_url=\"sqlite:///documents.db\"\n",
    "    )\n",
    "```\n",
    "index_to_docstore_id：文档存储的id，方便后面检索的时候，可以知道是哪个文档\n",
    "\"\"\"\n",
    "vector_store = FAISS.from_documents(embedding=embeding_model,documents=text_chunks,docstore=InMemoryDocstore(),index_to_docstore_id={})\n",
    "# 检索出结果\n",
    "vector_store.similarity_search(\"RAG是什么？\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化建议：我们使用RAG的过程中肯定是需要长期优化增删改查这个知识库的，所以我们需要把刚刚的初次生成的向量库保存下来，方便后面进行维护，下面我们用一个从头开始初始化向量库的代码来实践看看，而且如果我们是制作一个RAG的api，那不可能每次启动服务的时候都用embeding去初始化相同的文档去做嵌入，如果文本很多的情况下，会很耗费资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 初始化faiss索引\n",
    "\"\"\"\n",
    "为什么需要初始化索引？\n",
    "1. 类似于我们要创建一个表的时候要确定多少列，这样在后续查询的时候才知道我们只能查到多少列是有效数据\n",
    "2. 用helloworld来生成索引的维度，不是很重要，因为helloworld的维度是固定的，所以用helloworld来生成索引的维度，只是为了获取维度，后续我们用分块后的文本转换的向量来生成索引\n",
    "\"\"\"\n",
    "index = faiss.IndexFlatL2(len(embeding_model.embed_query(\"hello world\")))\n",
    "\n",
    "# 初始化向量库\n",
    "vector_store = FAISS(\n",
    "    # 引用刚刚初始化的embeding模型\n",
    "    embedding_function=embeding_model,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "# 向量库的写入方法（其他部分可以查看faiss的文档）\n",
    "vector_store.add_documents(documents=text_chunks)\n",
    "\n",
    "\n",
    "# 保存到本地\n",
    "folder_path = \"/data/xieyu/Teaching/RAG/data_retriever/rag_teach\"\n",
    "vector_store.save_local(index_name=\"rag_teach\",folder_path=folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/data/xieyu/Teaching/RAG/RAG的道与术.txt'}, page_content='RAG的道与术\\n目录\\n\\n目标读者\\n想要开始了解并学习 RAG技术的人\\n想要利用RAG实现智能知识问答的人\\nRAG是什么？\\n定义\\nRAG是一种能让大量知识 “活” 起来，人与知识无障碍对话的技术\\n背景\\nAI大模型已经能够和用户流畅交流，解读知识，总结文章，但由于训练他们的知识是互联网过往的公开内容，当我们有非公开的知识想让AI帮忙解读，总结时就涉及到他们的知识盲区，所以RAG技术就是让他们能够了解到我们的非公开知识，更加精准地回答我们的问题\\n场景\\n企业知识库\\n客服机器人\\n产品自动推荐\\nRAG主流方案\\n核心步骤\\n收集数据\\n导入数据\\n问题检索\\n结果评估\\n迭代优化\\n用dify进行演示\\n主流应用\\nlangchain\\n上手方便，装包随时可以使用\\ndbgpt\\n比较通用，api使用方便，原生支持graphRAG\\nragflow\\n文档处理最方便\\nQanything\\n检索到的重排序效果最好\\nfastgpt\\n接入模块易操作\\ndify\\n结合工作流更方便\\n制作对应的表格\\nRAG实践问题\\n核心目标\\n提高AI回复知识库相关问题的准确性\\n实践痛点\\n提问的问题稍微跟原文换一种说法就回答不上来\\n根据自己的理解问问题\\n根据自己的记忆关键词问问题\\n查询到的内容都不是自己要的内容\\n图片表格的知识信息无法查询\\n对长文本提问总结性问题总是回答不全\\n对于查询不到的问题进行编造强行回答\\n知识库特别大，难以完全测试来保证准确度\\n优化步骤\\n知道放什么知识？\\n知道怎么放知识？\\n知道怎么查知识？\\n知道查的对不对？\\n实践拆解\\n知道放什么知识？\\n范围\\n尽量只放入跟后续提问相关的内容，无效的内容越多，查询准确率越低\\n质量\\n内容要求要准确严谨，互相矛盾，避免大模型产生混乱\\n类型\\n本质上大模型只能够回答文本的内容，所以导入的文档应该尽量转化成普通文本模式\\n知识到怎么放知识\\n知识类型\\n纯文本导入\\n带图片文本导入\\n带表格文本导入\\n导入数据库\\nrelation-db\\nstr\\ngraph-db\\nnode\\nvector-db\\n转化成向量（世界通用语言）\\n知道怎么查知识？\\nquery_write\\n大模型改写\\nsearch_engine\\nvector\\nembeding\\nrerank\\nkeywords\\ngraphRAG\\n好问题+好引擎= 好答案\\n知道查的对不对？\\n检索内容相关度\\n大模型的理解表达正确度')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载本地向量库，并查询\n",
    "vector_store = FAISS.load_local(index_name=\"rag_teach\",folder_path=folder_path,embeddings=embeding_model,allow_dangerous_deserialization=True )\n",
    "vector_store.similarity_search(\"RAG是什么？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 知识库查询"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "刚刚不是已经演示过如何查询了吗？为什么还需要再分一个部分来讲？\n",
    "\n",
    "随着RAG技术的发展，很多人发现embeding模型在检索的时候，效果并不是很好，所以就有人提出了rerank模型，rerank模型可以理解为在检索后，对检索到的结果进行一个重新的排序，从而提高检索的准确率\n",
    "那关于rerank模型主要有3个问题:\n",
    "1. rerank模型是什么？跟embeding模型有什么区别？\n",
    "    其他rerank模型和embeding模型都是用来计算文本相似度的模型，但是他们的计算算法不同，rerank精准，但是速度慢，embeding速度快，但是效果不如rerank，因此现在基本上是先用embeding模型检索出文档范围，然后再用rerank模型进行排序，提高检索的精度，实现速度和精准度的平衡，如果想要了解他们的具体区别可以了解这位大佬的视频：https://www.bilibili.com/video/BV1r1421R77Y/?spm_id_from=333.337.search-card.all.click&vd_source=8b3e87015d9483289ee62c8c6c603927\n",
    "2. rerank模型怎么选?\n",
    "    参考embeding模型的选择\n",
    "3. rerank模型怎么用？\n",
    "    api方式或者langchain调用（想见代码）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/data/xieyu/Teaching/RAG/imgs/embeding+rerank.png\" alt=\"embeding+rerank\" width=\"30%\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lilk/RAG/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/data/xieyu/Teaching/RAG/RAG的道与术.txt'}, page_content='RAG的道与术\\n目录\\n\\n目标读者\\n想要开始了解并学习 RAG技术的人\\n想要利用RAG实现智能知识问答的人\\nRAG是什么？\\n定义\\nRAG是一种能让大量知识 “活” 起来，人与知识无障碍对话的技术\\n背景\\nAI大模型已经能够和用户流畅交流，解读知识，总结文章，但由于训练他们的知识是互联网过往的公开内容，当我们有非公开的知识想让AI帮忙解读，总结时就涉及到他们的知识盲区，所以RAG技术就是让他们能够了解到我们的非公开知识，更加精准地回答我们的问题\\n场景\\n企业知识库\\n客服机器人\\n产品自动推荐\\nRAG主流方案\\n核心步骤\\n收集数据\\n导入数据\\n问题检索\\n结果评估\\n迭代优化\\n用dify进行演示\\n主流应用\\nlangchain\\n上手方便，装包随时可以使用\\ndbgpt\\n比较通用，api使用方便，原生支持graphRAG\\nragflow\\n文档处理最方便\\nQanything\\n检索到的重排序效果最好\\nfastgpt\\n接入模块易操作\\ndify\\n结合工作流更方便\\n制作对应的表格\\nRAG实践问题\\n核心目标\\n提高AI回复知识库相关问题的准确性\\n实践痛点\\n提问的问题稍微跟原文换一种说法就回答不上来\\n根据自己的理解问问题\\n根据自己的记忆关键词问问题\\n查询到的内容都不是自己要的内容\\n图片表格的知识信息无法查询\\n对长文本提问总结性问题总是回答不全\\n对于查询不到的问题进行编造强行回答\\n知识库特别大，难以完全测试来保证准确度\\n优化步骤\\n知道放什么知识？\\n知道怎么放知识？\\n知道怎么查知识？\\n知道查的对不对？\\n实践拆解\\n知道放什么知识？\\n范围\\n尽量只放入跟后续提问相关的内容，无效的内容越多，查询准确率越低\\n质量\\n内容要求要准确严谨，互相矛盾，避免大模型产生混乱\\n类型\\n本质上大模型只能够回答文本的内容，所以导入的文档应该尽量转化成普通文本模式\\n知识到怎么放知识\\n知识类型\\n纯文本导入\\n带图片文本导入\\n带表格文本导入\\n导入数据库\\nrelation-db\\nstr\\ngraph-db\\nnode\\nvector-db\\n转化成向量（世界通用语言）\\n知道怎么查知识？\\nquery_write\\n大模型改写\\nsearch_engine\\nvector\\nembeding\\nrerank\\nkeywords\\ngraphRAG\\n好问题+好引擎= 好答案\\n知道查的对不对？\\n检索内容相关度\\n大模型的理解表达正确度')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "\n",
    "# 初始化retriever+rerank检索器\n",
    "vector_store = FAISS.load_local(index_name=\"rag_teach\",folder_path=folder_path,embeddings=embeding_model,allow_dangerous_deserialization=True )\n",
    "rerank_model = HuggingFaceCrossEncoder(model_name=\"/data/lilk/RAG/models/rerank/bge-reranker-v2-m3\")\n",
    "## 配置embeding模型返回25个，相似度阈值为0.001（有个小细节，相似度阈值的范围如果要控制是0-1，一定要在初始化embeding模型的时候设置normalize_embeddings=True）\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 25, \"score_threshold\": 0.001})\n",
    "## 配置rerank模型，返回10个\n",
    "compressor = CrossEncoderReranker(model=rerank_model, top_n=10)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "# 检索\n",
    "compression_retriever.get_relevant_documents(\"RAG是什么？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图检索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 为什么需要图检索？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入一个例子来说明：\n",
    "有一本书叫《小明与他的100个前女友》，总共10w字，然后如果我想要检索计算小明总共和多少个前女友亲亲过？\n",
    "\n",
    "【关键词检索】：这种时候会这10w的文章中检索出亲亲，然后把所有出现的亲亲的段落返回，这种情况会把一些跟小明无关的亲亲也返回，导致结果不准确，同时也有可能返回文本太长，导致结果不准确\n",
    "【向量检索】：向量检索也是查询出有出现亲亲的段落，但是好处就是可以把一些相关的行为，例如接吻的段落也返回，但是缺点就是如果文本量很大，检索的速度会很慢\n",
    "【图检索】：这种时候会构建一个图谱，图谱中包含小明、前女友、亲亲，然后检索的时候，会根据图谱中小明到前女友的边，然后返回所有小明到前女友的边，这样就可以避免关键词检索带来的问题，同时也可以把一些相关的行为，例如小明的接吻的段落都能精准返回，而且检索的速度也会很快\n",
    "\n",
    "总结为什么要用图检索？\n",
    "1. 图检索可以精准返回想要的结果\n",
    "2. 图检索可以检索出相关的行为\n",
    "3. 图检索的检索速度非常快\n",
    "\n",
    "图检索的缺点：\n",
    "要构建出图谱的关系，其实需要把分块的内容（就是书里面的每个章节）给大模型去分析关系，就会需要耗费比较多的token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 如何使用图检索？\n",
    "\n",
    "常见解决方案是：graphRAG；lightRAG；dbgpt的TugraphRAG，或者ragflow的graphRAG，目前测试下来lightRAG在准确度上和速度上都最强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 拉取lightRAG代码\n",
    "git clone https://github.com/HKUDS/LightRAG\n",
    "## 安装lightRAG依赖\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete, hf_model_complete, hf_embedding, openai_complete_if_cache\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 设置知识库的目录\n",
    "WORKING_DIR = \"./graphRAG\"\n",
    "\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)\n",
    "\n",
    "\n",
    "# 设置llm模型\n",
    "async def llm_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], **kwargs\n",
    ") -> str:\n",
    "    return await openai_complete_if_cache(\n",
    "        \"gpt-4o\",\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        api_key=\"your key\",\n",
    "        base_url=\"your base url\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "embedding_func=EmbeddingFunc(\n",
    "    embedding_dim=1024,\n",
    "    max_token_size=8000,\n",
    "    func=lambda texts: hf_embedding(\n",
    "        texts,\n",
    "        # 引用刚刚下载的embeding模型\n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"/data/lilk/RAG/models/embeding/bge-m3\"),\n",
    "        embed_model=AutoModel.from_pretrained(\"/data/lilk/RAG/models/embeding/bge-m3\")\n",
    "    )\n",
    ")\n",
    "\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=llm_model_func,  # Use gpt_4o_mini_complete LLM model\n",
    "    embedding_func=embedding_func,\n",
    "    # llm_model_func=gpt_4o_complete  # Optionally, use a stronger model\n",
    ")\n",
    "\n",
    "import textract\n",
    "\n",
    "# 把文档转化成txt插入到知识库中\n",
    "for filename in os.listdir(\"/data/xieyu/LightRAG/tugraph-doc\"):\n",
    "    filepath = os.path.join(\"/data/xieyu/LightRAG/tugraph-doc\", filename)\n",
    "    if os.path.isfile(filepath) and filepath.endswith('.pdf'):\n",
    "        text_content = textract.process(filepath)\n",
    "        rag.insert(text_content.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同的检索方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "a) Naive 搜索（朴素搜索）：\n",
    "    这是最简单的搜索方法。\n",
    "    它可能会直接在整个数据集中进行关键词匹配或简单的相似度计算。\n",
    "    速度可能较快，但准确性可能较低。\n",
    "b) Local 搜索（局部搜索）：\n",
    "    这种方法可能会先定位到图中的某个局部区域，然后在该区域内进行搜索。\n",
    "    适合于当我们大致知道答案可能在哪个区域时使用。\n",
    "    可以提高搜索效率，但如果初始定位不准确，可能会错过重要信息。\n",
    "c) Global 搜索（全局搜索）：\n",
    "    这种方法会在整个图结构中进行搜索。\n",
    "    相比局部搜索，它可能会更全面，但也可能更耗时。\n",
    "    适合于需要考虑整个数据集的复杂查询。\n",
    "d) Hybrid 搜索（混合搜索）：\n",
    "    这种方法结合了其他搜索策略的优点。\n",
    "    可能会先进行全局搜索来定位相关区域，然后在这些区域进行更详细的局部搜索。\n",
    "    通常能在效率和准确性之间取得较好的平衡。\n",
    "\"\"\"\n",
    "\n",
    "# Perform naive search\n",
    "print(rag.query(\"如何查询数据库中现有角色及其相关信息？\", param=QueryParam(mode=\"naive\")))\n",
    "\n",
    "# Perform local search\n",
    "print(rag.query(\"这些文档主要的内容是什么？\", param=QueryParam(mode=\"local\")))\n",
    "\n",
    "# Perform global search\n",
    "print(rag.query(\"这些文档主要的内容是什么？\", param=QueryParam(mode=\"global\")))\n",
    "\n",
    "# Perform hybrid search\n",
    "print(rag.query(\"这些文档主要的内容是什么？\", param=QueryParam(mode=\"hybrid\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
