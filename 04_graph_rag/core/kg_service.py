"""çŸ¥è¯†å›¾è°±æœåŠ¡ - æ¨¡å—åŒ–ç‰ˆæœ¬"""

import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict

try:
    from ..vector_db.factory import VectorDBFactory
    from ..document_processor.file_importer import FileImporter
    from ..knowledge_extractor.knowledge_extractor import KnowledgeExtractor
    from ..storage.graph_storage import GraphStorage
    from ..storage.db_storage import DBStorage
    from ..storage.vector_storage import VectorStorage
    from ..query.graphrag_query import GraphRAGQueryEngine
    from ..query.keyword_extractor import KeywordExtractor
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os
    current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)
    from vector_db.factory import VectorDBFactory
    from document_processor.file_importer import FileImporter
    from knowledge_extractor.knowledge_extractor import KnowledgeExtractor
    from storage.graph_storage import GraphStorage
    from storage.db_storage import DBStorage
    from storage.vector_storage import VectorStorage
    from query.graphrag_query import GraphRAGQueryEngine
    from query.keyword_extractor import KeywordExtractor
from .config import GraphRAGConfig
from app.core.config.config import MilvusDBName, MilvusCollectionName, LlmClientType
from app.embeddings.milvus_client import MilvusManager

logger = logging.getLogger(__name__)


class KGService:
    """çŸ¥è¯†å›¾è°±æœåŠ¡ç±» - æ¨¡å—åŒ–ç‰ˆæœ¬
    
    ä½¿ç”¨ä¾èµ–æ³¨å…¥ç»„åˆå„ä¸ªæ¨¡å—ï¼Œæä¾›é«˜çº§APIã€‚
    """

    def __init__(
        self,
        config: Optional[GraphRAGConfig] = None,
        vector_db=None,
        document_processor=None,
        knowledge_extractor=None,
        graph_storage=None,
        db_storage=None,
        vector_storage=None,
        query_engine=None,
        model_type: Optional[LlmClientType] = None,
        milvus_manager=None,
    ):
        """åˆå§‹åŒ–KGService
        
        Args:
            config: é…ç½®å¯¹è±¡
            vector_db: å‘é‡æ•°æ®åº“é€‚é…å™¨ï¼ˆå¯é€‰ï¼Œä¼šæ ¹æ®configåˆ›å»ºï¼‰
            document_processor: æ–‡æ¡£å¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
            knowledge_extractor: çŸ¥è¯†æŠ½å–å™¨ï¼ˆå¯é€‰ï¼‰
            graph_storage: å›¾è°±å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
            db_storage: æ•°æ®åº“å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
            vector_storage: å‘é‡å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
            query_engine: æŸ¥è¯¢å¼•æ“ï¼ˆå¯é€‰ï¼‰
            model_type: LLMå®¢æˆ·ç«¯ç±»å‹ï¼ˆå‘åå…¼å®¹ï¼‰
            milvus_manager: MilvusManagerå®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰
        """
        # é…ç½®
        self.config = config or GraphRAGConfig()
        
        # å¦‚æœæä¾›äº†model_typeï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œæ›´æ–°config
        if model_type:
            self.config.model_type = model_type

        # å‘é‡æ•°æ®åº“
        if vector_db is None:
            if milvus_manager:
                # å‘åå…¼å®¹ï¼šä½¿ç”¨æä¾›çš„MilvusManager
                try:
                    from ..vector_db.milvus_adapter import MilvusAdapter
                except ImportError:
                    from vector_db.milvus_adapter import MilvusAdapter
                vector_db = MilvusAdapter(milvus_manager=milvus_manager)
            else:
                vector_db = VectorDBFactory.create(
                    self.config.vector_db_type,
                    self.config.vector_db_config,
                    milvus_manager=milvus_manager,
                )
        self.vector_db = vector_db

        # å‘é‡å­˜å‚¨
        self.vector_storage = vector_storage or VectorStorage(
            vector_db=self.vector_db
        )

        # å›¾è°±å­˜å‚¨
        self.graph_storage = graph_storage or GraphStorage(
            graph_storage_dir=self.config.graph_storage_dir
        )

        # æ•°æ®åº“å­˜å‚¨
        self.db_storage = db_storage or DBStorage()

        # æ–‡æ¡£å¤„ç†å™¨
        self.document_processor = document_processor or FileImporter()

        # çŸ¥è¯†æŠ½å–å™¨ï¼ˆæ”¯æŒæç¤ºè¯é…ç½®ï¼‰
        if knowledge_extractor is None:
            # æ£€æŸ¥configä¸­æ˜¯å¦æœ‰æç¤ºè¯é…ç½®
            prompt_file_path = getattr(self.config, 'prompt_file_path', None)
            if prompt_file_path:
                self.knowledge_extractor = KnowledgeExtractor(
                    model_type=self.config.model_type,
                    prompt_file_path=prompt_file_path
                )
            else:
                self.knowledge_extractor = KnowledgeExtractor(
                    model_type=self.config.model_type
                )
        else:
            self.knowledge_extractor = knowledge_extractor

        # æŸ¥è¯¢å¼•æ“
        self.query_engine = query_engine or GraphRAGQueryEngine(
            vector_storage=self.vector_storage,
            graph_storage=self.graph_storage,
            db_storage=self.db_storage,
            keyword_extractor=KeywordExtractor(model_type=self.config.model_type),
        )

        # å‘åå…¼å®¹ï¼šä¿ç•™åŸæœ‰çš„å±æ€§
        self.milvus_manager = milvus_manager or self._get_milvus_manager()
        self.embedding_func = self.vector_storage.embedding_func
        self.model_type = self.config.model_type
        self.knowledge_graph = self.graph_storage.get_graph()

    def _get_milvus_manager(self):
        """è·å–MilvusManagerå®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
        if isinstance(self.vector_db, type(self.vector_db)):
            # å¦‚æœæ˜¯MilvusAdapterï¼Œå°è¯•è·å–å†…éƒ¨çš„MilvusManager
            if hasattr(self.vector_db, 'milvus_manager'):
                return self.vector_db.milvus_manager
        # åˆ›å»ºæ–°çš„MilvusManager
        return MilvusManager()

    async def process_document_chunks(
        self, document_id: int, chunks: List[Dict[str, Any]], max_concurrent: int = 5
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£çš„æ‰€æœ‰chunksï¼ŒæŠ½å–å®ä½“å’Œå…³ç³»å¹¶å…¥åº“

        Args:
            document_id: æ–‡æ¡£ID
            chunks: chunkåˆ—è¡¨ï¼Œæ¯ä¸ªchunkåŒ…å« {"text": "...", "chunk_id": "...", "tokens": 123}
            max_concurrent: æœ€å¤§å¹¶å‘LLMè°ƒç”¨æ•°

        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            logger.info(
                f"å¼€å§‹å¤„ç†æ–‡æ¡£ {document_id} çš„ {len(chunks)} ä¸ªchunksï¼Œå¹¶å‘æ•°: {max_concurrent}"
            )

            # åˆå§‹åŒ–å…ƒæ•°æ®
            await self.db_storage.init_kg_metadata(document_id)

            # å¹¶å‘æŠ½å–æ‰€æœ‰chunksçš„å®ä½“å’Œå…³ç³»
            all_entities, all_relations = (
                await self._batch_extract_from_chunks_optimized(
                    chunks, document_id, max_concurrent
                )
            )

            # åˆå¹¶å’Œå…¥åº“
            entity_count = await self._merge_and_save_entities(
                document_id, all_entities
            )
            relation_count = await self._merge_and_save_relations(
                document_id, all_relations
            )

            # ä¿å­˜chunksåˆ°å‘é‡åº“
            await self.vector_storage.batch_save_chunks(
                document_id,
                chunks,
                self.config.db_name,
                self.config.chunk_collection,
            )

            # ä¿å­˜NetworkXå›¾åˆ°æ–‡ä»¶
            self.graph_storage.save()

            await self.db_storage.update_kg_metadata(
                document_id, entity_count, relation_count, "ready"
            )

            result = {
                "document_id": document_id,
                "chunks_processed": len(chunks),
                "entities_extracted": entity_count,
                "relations_extracted": relation_count,
                "status": "success",
            }

            logger.info(f"æ–‡æ¡£ {document_id} å¤„ç†å®Œæˆ: {result}")
            return result

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£ {document_id} æ—¶å‡ºé”™: {str(e)}")
            await self.db_storage.update_kg_metadata(document_id, 0, 0, "error")
            raise

    async def _batch_extract_from_chunks_optimized(
        self, chunks: List[Dict[str, Any]], document_id: int, max_concurrent: int = 5
    ) -> Tuple[Dict, Dict]:
        """å¹¶å‘æ‰¹é‡æŠ½å–chunksçš„å®ä½“å’Œå…³ç³»"""
        try:
            logger.info(
                f"ğŸš€ ä¼˜åŒ–å¹¶å‘æŠ½å– {len(chunks)} ä¸ªchunksï¼Œå¹¶å‘æ•°: {max_concurrent}"
            )

            semaphore = asyncio.Semaphore(max_concurrent)
            completed_chunks = 0
            completed_lock = asyncio.Lock()

            async def extract_with_semaphore(chunk, index):
                nonlocal completed_chunks
                async with semaphore:
                    chunk_id = chunk.get("chunk_id", self._generate_chunk_id(chunk.get("text", "")))
                    content = chunk.get("text", "")

                    try:
                        entities, relations = await self.knowledge_extractor.extract(
                            content, chunk_id, document_id
                        )

                        async with completed_lock:
                            completed_chunks += 1
                            progress = (completed_chunks / len(chunks)) * 100

                        print(
                            f"         âœ… å®Œæˆ {index+1}/{len(chunks)} ({progress:.1f}%): å®ä½“={len(entities)}, å…³ç³»={len(relations)}"
                        )
                        return entities, relations

                    except Exception as e:
                        async with completed_lock:
                            completed_chunks += 1

                        print(
                            f"         âŒ å¤±è´¥ {index+1}/{len(chunks)}: {str(e)[:50]}..."
                        )
                        logger.error(f"Chunk {chunk_id} æŠ½å–å¤±è´¥: {e}")
                        return {}, {}

            # åˆ›å»ºæ‰€æœ‰å¹¶å‘ä»»åŠ¡
            tasks = [
                asyncio.create_task(extract_with_semaphore(chunk, i))
                for i, chunk in enumerate(chunks)
            ]

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # åˆå¹¶æ‰€æœ‰ç»“æœ
            all_entities = defaultdict(list)
            all_relations = defaultdict(list)

            successful_extractions = 0

            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Chunk {i+1} å¤„ç†å¼‚å¸¸: {result}")
                    continue

                entities, relations = result
                if entities or relations:
                    successful_extractions += 1

                # å®ä½“æŒ‰(åç§°, ç±»å‹)åˆ†ç»„
                for entity_name, entity_data_list in entities.items():
                    for entity_data in entity_data_list:
                        entity_key = (
                            entity_data["entity_name"],
                            entity_data["entity_type"],
                        )
                        all_entities[entity_key].append(entity_data)

                # å…³ç³»æŒ‰(æº, ç›®æ ‡, ç±»å‹)åˆ†ç»„
                for relation_key, relation_data_list in relations.items():
                    for relation_data in relation_data_list:
                        enhanced_key = (
                            relation_data["source_entity"],
                            relation_data["target_entity"],
                            relation_data["relation_type"],
                        )
                        all_relations[enhanced_key].append(relation_data)

            logger.info(
                f"âœ… ä¼˜åŒ–å¹¶å‘æŠ½å–å®Œæˆ: {successful_extractions}/{len(chunks)} ä¸ªchunkæˆåŠŸ"
            )

            return dict(all_entities), dict(all_relations)

        except Exception as e:
            logger.error(f"æ‰¹é‡æŠ½å–å¤±è´¥: {str(e)}")
            raise

    async def _merge_and_save_entities(
        self, document_id: int, entities_dict: Dict[Tuple[str, str], List[Dict]]
    ) -> int:
        """åˆå¹¶å¹¶ä¿å­˜å®ä½“åˆ°æ•°æ®åº“å’Œå‘é‡åº“"""
        saved_count = 0
        merged_entities = []

        # æ‰¹é‡åˆå¹¶å®ä½“æ•°æ®
        for entity_key, entity_data_list in entities_dict.items():
            try:
                # ç®€åŒ–åˆå¹¶é€»è¾‘ï¼ˆå®é™…åº”è¯¥è°ƒç”¨_merge_entity_dataï¼‰
                merged_entity = entity_data_list[0].copy()
                # åˆå¹¶æè¿°
                descriptions = [e.get("description", "") for e in entity_data_list if e.get("description")]
                merged_entity["description"] = " | ".join(sorted(set(descriptions))) if descriptions else ""

                # ä¿å­˜åˆ°å…³ç³»æ•°æ®åº“
                await self.db_storage.save_entity(merged_entity)

                # ä¿å­˜åˆ°NetworkXå›¾
                self.graph_storage.add_entity(merged_entity)

                merged_entities.append(merged_entity)
                saved_count += 1

            except Exception as e:
                logger.warning(f"è·³è¿‡å®ä½“ '{entity_key}': {str(e)}")
                continue

        # æ‰¹é‡ä¿å­˜åˆ°å‘é‡åº“
        if merged_entities:
            await self.vector_storage.batch_save_entities(
                merged_entities,
                self.config.db_name,
                self.config.entity_collection,
            )

        logger.info(f"ä¿å­˜äº† {saved_count} ä¸ªå®ä½“åˆ°æ•°æ®åº“å’Œå‘é‡åº“")
        return saved_count

    async def _merge_and_save_relations(
        self, document_id: int, relations_dict: Dict[Tuple, List[Dict]]
    ) -> int:
        """åˆå¹¶å¹¶ä¿å­˜å…³ç³»åˆ°æ•°æ®åº“å’Œå‘é‡åº“"""
        saved_count = 0
        merged_relations = []

        # æ‰¹é‡åˆå¹¶å…³ç³»æ•°æ®
        for relation_data_list in relations_dict.values():
            try:
                # ç®€åŒ–åˆå¹¶é€»è¾‘
                merged_relation = relation_data_list[0].copy()
                # åˆå¹¶æè¿°å’Œæƒé‡
                descriptions = [r.get("description", "") for r in relation_data_list if r.get("description")]
                merged_relation["description"] = " | ".join(sorted(set(descriptions))) if descriptions else ""
                merged_relation["weight"] = sum(r.get("weight", 1.0) for r in relation_data_list)

                # ä¿å­˜åˆ°å…³ç³»æ•°æ®åº“
                await self.db_storage.save_relation(merged_relation)

                # ä¿å­˜åˆ°NetworkXå›¾
                self.graph_storage.add_relation(merged_relation)

                merged_relations.append(merged_relation)
                saved_count += 1

            except Exception as e:
                logger.warning(f"è·³è¿‡å…³ç³»: {str(e)}")
                continue

        # æ‰¹é‡ä¿å­˜åˆ°å‘é‡åº“
        if merged_relations:
            await self.vector_storage.batch_save_relations(
                merged_relations,
                self.config.db_name,
                self.config.relation_collection,
            )

        logger.info(f"ä¿å­˜äº† {saved_count} ä¸ªå…³ç³»åˆ°æ•°æ®åº“å’Œå‘é‡åº“")
        return saved_count

    async def graphrag_query(
        self, query: str, mode: str = "mix", top_k: int = 10
    ) -> Dict[str, Any]:
        """GraphRAGæŸ¥è¯¢æ¥å£ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return await self.query_engine.graphrag_query(query, mode, top_k)

    # å‘åå…¼å®¹æ–¹æ³•
    async def extract_keywords_from_query(self, query: str) -> Tuple[List[str], List[str]]:
        """æå–å…³é”®è¯ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return await self.query_engine.keyword_extractor.extract_keywords(query)

    def _generate_chunk_id(self, content: str) -> str:
        """ç”Ÿæˆchunk ID"""
        import hashlib
        return f"chunk-{hashlib.md5(content.encode('utf-8')).hexdigest()[:16]}"

