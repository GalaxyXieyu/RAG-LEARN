# å¤šæ¨¡æ€åµŒå…¥ä¸æ£€ç´¢ ğŸ¨

> çªç ´æ–‡æœ¬å±€é™ï¼Œè®©AIç†è§£å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘

## ğŸ“– ç« èŠ‚æ¦‚è¿°

æœ¬ç« èŠ‚æ¢ç´¢**å¤šæ¨¡æ€åµŒå…¥ï¼ˆMultimodal Embeddingï¼‰**æŠ€æœ¯ï¼Œå­¦ä¹ å¦‚ä½•è®©AIåŒæ—¶ç†è§£å’Œå¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šç§æ¨¡æ€çš„æ•°æ®ã€‚è¿™æ˜¯RAGæŠ€æœ¯çš„é‡è¦æ‰©å±•æ–¹å‘ï¼Œèƒ½å¤Ÿå®ç°æ›´ä¸°å¯Œçš„åº”ç”¨åœºæ™¯ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« èŠ‚å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- âœ… ç†è§£å¤šæ¨¡æ€åµŒå…¥çš„æ ¸å¿ƒåŸç†
- âœ… æŒæ¡CLIPã€ImageBindç­‰ä¸»æµæ¨¡å‹
- âœ… å®ç°ä»¥å›¾æœå›¾ã€ä»¥æ–‡æœå›¾åŠŸèƒ½
- âœ… æ„å»ºå¤šæ¨¡æ€RAGç³»ç»Ÿ
- âœ… å¤„ç†è§†é¢‘å’ŒéŸ³é¢‘æ£€ç´¢åœºæ™¯

---

## ä¸€ã€ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€åµŒå…¥ï¼Ÿ

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

**å•æ¨¡æ€ vs å¤šæ¨¡æ€**ï¼š

```
ä¼ ç»ŸåµŒå…¥ï¼ˆå•æ¨¡æ€ï¼‰ï¼š
æ–‡æœ¬ â†’ Text Encoder â†’ å‘é‡ [0.2, 0.5, ...]
å›¾åƒ â†’ Image Encoder â†’ å‘é‡ [0.8, 0.1, ...]
é—®é¢˜ï¼šä¸¤ä¸ªå‘é‡åœ¨ä¸åŒç©ºé—´ï¼Œæ— æ³•ç›´æ¥æ¯”è¾ƒ

å¤šæ¨¡æ€åµŒå…¥ï¼š
æ–‡æœ¬ â†’ Multimodal Encoder â†’ ç»Ÿä¸€å‘é‡ç©ºé—´
å›¾åƒ â†’ Multimodal Encoder â†’ ç»Ÿä¸€å‘é‡ç©ºé—´
ä¼˜åŠ¿ï¼šæ–‡æœ¬å’Œå›¾åƒå‘é‡å¯ä»¥ç›´æ¥æ¯”è¾ƒç›¸ä¼¼åº¦ï¼
```

### 1.2 åº”ç”¨åœºæ™¯

```
1. ä»¥å›¾æœå›¾ (Image-to-Image Search)
   ä¸Šä¼ ä¸€å¼ äº§å“å›¾ â†’ æ‰¾åˆ°ç›¸ä¼¼äº§å“

2. ä»¥æ–‡æœå›¾ (Text-to-Image Search)
   è¾“å…¥"çº¢è‰²çš„è·‘è½¦" â†’ æ£€ç´¢å‡ºæ‰€æœ‰çº¢è‰²è·‘è½¦å›¾ç‰‡

3. ä»¥å›¾æœæ–‡ (Image-to-Text Search)
   ä¸Šä¼ åœºæ™¯å›¾ç‰‡ â†’ æ‰¾åˆ°ç›¸å…³çš„æ–‡ç« å’Œæè¿°

4. è§†é¢‘æ£€ç´¢ (Video Search)
   è¾“å…¥"è¿›çƒç¬é—´" â†’ æ£€ç´¢å‡ºè¶³çƒæ¯”èµ›ä¸­çš„è¿›çƒç‰‡æ®µ

5. éŸ³é¢‘æ£€ç´¢ (Audio Search)
   å“¼ä¸€æ®µæ—‹å¾‹ â†’ æ‰¾åˆ°å®Œæ•´æ­Œæ›²

6. å¤šæ¨¡æ€é—®ç­”
   "è¿™å¼ å›¾ç‰‡é‡Œçš„äººåœ¨åšä»€ä¹ˆï¼Ÿ" â†’ ç†è§£å›¾ç‰‡å¹¶å›ç­”

7. è·¨æ¨¡æ€ç”Ÿæˆ
   æ–‡æœ¬ â†’ ç”Ÿæˆå›¾åƒï¼ˆDALL-Eï¼‰
   å›¾åƒ â†’ ç”Ÿæˆæè¿°
```

---

## äºŒã€ä¸»æµå¤šæ¨¡æ€æ¨¡å‹

### 2.1 CLIPï¼ˆContrastive Language-Image Pre-trainingï¼‰

**å¼€å‘è€…**ï¼šOpenAI  
**ç‰¹ç‚¹**ï¼šæ–‡æœ¬-å›¾åƒè”åˆåµŒå…¥çš„å¼€åˆ›æ€§å·¥ä½œ

**æ ¸å¿ƒåŸç†**ï¼š
```python
# CLIPçš„å¯¹æ¯”å­¦ä¹ æœºåˆ¶
æ–‡æœ¬ï¼š"ä¸€åªçŒ«ååœ¨æ²™å‘ä¸Š"    â†’ Text Encoder  â†’ å‘é‡A
å›¾åƒï¼š[çŒ«ååœ¨æ²™å‘ä¸Šçš„ç…§ç‰‡]  â†’ Image Encoder â†’ å‘é‡B

è®­ç»ƒç›®æ ‡ï¼š
- åŒ¹é…çš„æ–‡æœ¬-å›¾åƒå¯¹ â†’ å‘é‡è·ç¦»è¿‘
- ä¸åŒ¹é…çš„æ–‡æœ¬-å›¾åƒå¯¹ â†’ å‘é‡è·ç¦»è¿œ
```

**ä¼˜åŠ¿**ï¼š
- âœ… é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆä¸éœ€è¦è®­ç»ƒå³å¯åˆ†ç±»æ–°ç±»åˆ«ï¼‰
- âœ… æ–‡æœ¬-å›¾åƒè·¨æ¨¡æ€æ£€ç´¢
- âœ… å¼€æºï¼Œæ˜“äºä½¿ç”¨

**åº”ç”¨ç¤ºä¾‹**ï¼š
```python
from transformers import CLIPProcessor, CLIPModel
import torch
from PIL import Image

# åŠ è½½æ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# æ–‡æœ¬å’Œå›¾åƒç¼–ç 
text_inputs = processor(text=["ä¸€åªçŒ«", "ä¸€åªç‹—"], return_tensors="pt", padding=True)
image = Image.open("cat.jpg")
image_inputs = processor(images=image, return_tensors="pt")

# è·å–åµŒå…¥
with torch.no_grad():
    text_embeddings = model.get_text_features(**text_inputs)
    image_embeddings = model.get_image_features(**image_inputs)

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = torch.cosine_similarity(text_embeddings, image_embeddings)
print(f"æ–‡æœ¬'ä¸€åªçŒ«'ä¸å›¾åƒçš„ç›¸ä¼¼åº¦: {similarity[0].item():.4f}")
```

### 2.2 ImageBindï¼ˆOne Embedding Space To Bind Them Allï¼‰

**å¼€å‘è€…**ï¼šMeta AI  
**ç‰¹ç‚¹**ï¼šæ”¯æŒ6ç§æ¨¡æ€çš„ç»Ÿä¸€åµŒå…¥ç©ºé—´

**æ”¯æŒçš„æ¨¡æ€**ï¼š
1. æ–‡æœ¬ï¼ˆTextï¼‰
2. å›¾åƒï¼ˆImageï¼‰
3. éŸ³é¢‘ï¼ˆAudioï¼‰
4. æ·±åº¦å›¾ï¼ˆDepthï¼‰
5. çƒ­æˆåƒï¼ˆThermalï¼‰
6. IMUæ•°æ®ï¼ˆè¿åŠ¨ä¼ æ„Ÿå™¨ï¼‰

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
```
ä»»æ„æ¨¡æ€ â†” ä»»æ„æ¨¡æ€çš„æ£€ç´¢
ä¾‹å¦‚ï¼š
- å£°éŸ³ â†’ å›¾åƒï¼ˆå¬åˆ°æµ·æµªå£° â†’ æ‰¾åˆ°æµ·æ»©å›¾ç‰‡ï¼‰
- æ–‡æœ¬ â†’ éŸ³é¢‘ï¼ˆ"é›·å£°" â†’ æ‰¾åˆ°æ‰“é›·çš„éŸ³é¢‘ï¼‰
- å›¾åƒ â†’ æ·±åº¦å›¾
```

### 2.3 å…¶ä»–é‡è¦æ¨¡å‹

| æ¨¡å‹ | å¼€å‘è€… | æ¨¡æ€æ”¯æŒ | ç‰¹ç‚¹ |
|------|-------|----------|------|
| **ALIGN** | Google | æ–‡æœ¬+å›¾åƒ | æ›´å¤§è§„æ¨¡è®­ç»ƒï¼ˆ18äº¿å¯¹ï¼‰ |
| **Florence** | Microsoft | æ–‡æœ¬+å›¾åƒ | ç»Ÿä¸€çš„è§†è§‰åŸºç¡€æ¨¡å‹ |
| **Jina CLIP** | Jina AI | æ–‡æœ¬+å›¾åƒ | æ”¯æŒå¤šè¯­è¨€ï¼Œä¸­æ–‡å‹å¥½ |
| **Chinese CLIP** | OFA-Sys | æ–‡æœ¬+å›¾åƒ | ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ– |
| **BEiT-3** | Microsoft | æ–‡æœ¬+å›¾åƒ | Vision-Languageé¢„è®­ç»ƒ |

---

## ä¸‰ã€å¤šæ¨¡æ€RAGæ¶æ„

### 3.1 ä¼ ç»ŸRAG vs å¤šæ¨¡æ€RAG

**ä¼ ç»Ÿæ–‡æœ¬RAG**ï¼š
```
ç”¨æˆ·æé—®(æ–‡æœ¬) â†’ æ–‡æœ¬æ£€ç´¢ â†’ æ–‡æœ¬ç‰‡æ®µ â†’ LLM â†’ æ–‡æœ¬ç­”æ¡ˆ
```

**å¤šæ¨¡æ€RAG**ï¼š
```
ç”¨æˆ·æé—®(æ–‡æœ¬/å›¾åƒ/è¯­éŸ³) 
    â†“
å¤šæ¨¡æ€æ£€ç´¢(æ–‡æœ¬+å›¾åƒ+è§†é¢‘+éŸ³é¢‘)
    â†“
å¤šæ¨¡æ€å†…å®¹(æ–‡æœ¬æè¿° + ç›¸å…³å›¾ç‰‡ + è§†é¢‘ç‰‡æ®µ)
    â†“
å¤šæ¨¡æ€LLM (GPT-4V, LLaVA, Qwen-VL)
    â†“
å¯Œæ–‡æœ¬ç­”æ¡ˆ(æ–‡å­— + å›¾ç‰‡ + é“¾æ¥)
```

### 3.2 ç³»ç»Ÿæ¶æ„ç¤ºä¾‹

```python
# å¤šæ¨¡æ€RAGç³»ç»Ÿä¼ªä»£ç 

class MultimodalRAG:
    def __init__(self):
        self.text_embedder = OpenAIEmbedding()
        self.image_embedder = CLIPModel()
        self.video_embedder = VideoEmbedder()
        self.vector_db = MilvusClient()
        self.multimodal_llm = GPT4V()
    
    def index_document(self, doc):
        """ç´¢å¼•å¤šæ¨¡æ€æ–‡æ¡£"""
        # 1. æå–æ–‡æœ¬å†…å®¹
        text_chunks = self.chunk_text(doc.text)
        text_vectors = self.text_embedder.encode(text_chunks)
        
        # 2. æå–å›¾åƒ
        images = self.extract_images(doc)
        image_vectors = self.image_embedder.encode(images)
        
        # 3. æå–è§†é¢‘å…³é”®å¸§
        if doc.has_video():
            frames = self.extract_key_frames(doc.video)
            frame_vectors = self.video_embedder.encode(frames)
        
        # 4. å­˜å…¥å‘é‡åº“ï¼ˆä¸åŒCollectionæˆ–ç”¨æ ‡ç­¾åŒºåˆ†ï¼‰
        self.vector_db.insert("text_collection", text_vectors, metadata=...)
        self.vector_db.insert("image_collection", image_vectors, metadata=...)
    
    def search(self, query, query_type="text"):
        """å¤šæ¨¡æ€æ£€ç´¢"""
        if query_type == "text":
            # æ–‡æœ¬æŸ¥è¯¢ â†’ æ£€ç´¢æ–‡æœ¬ + å›¾åƒ
            text_results = self.search_text(query)
            image_results = self.search_images_by_text(query)
        
        elif query_type == "image":
            # å›¾åƒæŸ¥è¯¢ â†’ æ£€ç´¢ç›¸ä¼¼å›¾åƒ + ç›¸å…³æ–‡æœ¬
            image_results = self.search_images_by_image(query)
            text_results = self.search_text_by_image(query)
        
        return self.merge_results(text_results, image_results)
    
    def generate_answer(self, query, retrieved_contents):
        """å¤šæ¨¡æ€ç­”æ¡ˆç”Ÿæˆ"""
        # æ„å»ºå¤šæ¨¡æ€prompt
        prompt = {
            "text": query,
            "images": [content.image for content in retrieved_contents if content.has_image()],
            "context": [content.text for content in retrieved_contents]
        }
        
        # è°ƒç”¨å¤šæ¨¡æ€LLM
        answer = self.multimodal_llm.generate(prompt)
        return answer
```

---

## å››ã€å®æˆ˜åº”ç”¨åœºæ™¯

### 4.1 ç”µå•†äº§å“æœç´¢

**éœ€æ±‚**ï¼šç”¨æˆ·ä¸Šä¼ ä¸€å¼ è¡£æœç…§ç‰‡ï¼Œæ‰¾åˆ°ç›¸ä¼¼çš„åœ¨å”®å•†å“

**å®ç°æ–¹æ¡ˆ**ï¼š
```python
# 1. å•†å“åº“ç´¢å¼•
products = load_products()  # åŒ…å«å›¾ç‰‡å’Œæè¿°
for product in products:
    # å›¾åƒåµŒå…¥
    image_embedding = clip_model.encode_image(product.image)
    # æ–‡æœ¬åµŒå…¥
    text_embedding = clip_model.encode_text(product.description)
    # å­˜å‚¨
    vector_db.insert({
        "product_id": product.id,
        "image_vector": image_embedding,
        "text_vector": text_embedding,
        "price": product.price,
        "category": product.category
    })

# 2. ç”¨æˆ·æœç´¢
user_image = upload_image()
query_vector = clip_model.encode_image(user_image)

# 3. æ£€ç´¢ç›¸ä¼¼å•†å“
results = vector_db.search(
    collection="products",
    query_vector=query_vector,
    top_k=10,
    filter="price < 500 AND category == 'clothing'"
)

# 4. å±•ç¤ºç»“æœ
for result in results:
    print(f"å•†å“: {result.name}, ç›¸ä¼¼åº¦: {result.score:.2%}")
    display(result.image)
```

### 4.2 æ™ºèƒ½ç›¸å†Œç®¡ç†

**éœ€æ±‚**ï¼šé€šè¿‡æ–‡æœ¬æè¿°æ‰¾åˆ°ç›¸å†Œä¸­çš„ç…§ç‰‡

**ç¤ºä¾‹æŸ¥è¯¢**ï¼š
- "æˆ‘å’Œå°æ˜åœ¨æµ·è¾¹çš„åˆå½±"
- "2023å¹´æ˜¥èŠ‚çš„ç…§ç‰‡"
- "æœ‰å°ç‹—çš„ç…§ç‰‡"
- "å¤•é˜³çš„é£æ™¯ç…§"

**å®ç°è¦ç‚¹**ï¼š
```python
# 1. ç…§ç‰‡é¢„å¤„ç†
for photo in album:
    # æå–å›¾åƒç‰¹å¾
    image_vector = clip_model.encode_image(photo)
    
    # æå–å…ƒæ•°æ®
    metadata = {
        "date": photo.exif_data.date,
        "location": photo.exif_data.gps,
        "people": detect_faces(photo),  # äººè„¸è¯†åˆ«
        "objects": detect_objects(photo),  # ç‰©ä½“æ£€æµ‹
    }
    
    # å­˜å‚¨
    save_to_vector_db(image_vector, metadata)

# 2. æ–‡æœ¬æœç´¢
query = "æˆ‘å’Œå°æ˜åœ¨æµ·è¾¹çš„åˆå½±"
query_vector = clip_model.encode_text(query)
results = vector_db.search(query_vector, filter="people CONTAINS 'å°æ˜'")
```

### 4.3 è§†é¢‘å†…å®¹æ£€ç´¢

**éœ€æ±‚**ï¼šåœ¨é•¿è§†é¢‘ä¸­æ‰¾åˆ°ç‰¹å®šåœºæ™¯

**å®ç°æ–¹æ¡ˆ**ï¼š
```python
# 1. è§†é¢‘é¢„å¤„ç†
video = load_video("lecture.mp4")
frames = extract_frames(video, fps=1)  # æ¯ç§’æå–1å¸§

for i, frame in enumerate(frames):
    # å¸§åµŒå…¥
    frame_vector = clip_model.encode_image(frame)
    
    # å¦‚æœæœ‰å­—å¹•ï¼Œä¹Ÿç¼–ç 
    if has_subtitle(video, timestamp=i):
        subtitle_text = get_subtitle(video, i)
        text_vector = clip_model.encode_text(subtitle_text)
    
    # å­˜å‚¨ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    vector_db.insert({
        "video_id": video.id,
        "timestamp": i,
        "frame_vector": frame_vector,
        "subtitle_vector": text_vector
    })

# 2. åœºæ™¯æœç´¢
query = "è®²åˆ°å‘é‡æ•°æ®åº“çš„éƒ¨åˆ†"
query_vector = clip_model.encode_text(query)
results = vector_db.search(query_vector, filter="video_id == 'lecture'")

# 3. å®šä½æ—¶é—´ç‚¹
for result in results:
    print(f"æ‰¾åˆ°ç›¸å…³åœºæ™¯: {result.timestamp}ç§’")
    video.seek(result.timestamp)
```

### 4.4 åŒ»å­¦å½±åƒæ£€ç´¢

**éœ€æ±‚**ï¼šæ ¹æ®ç—…ç—‡æè¿°æ‰¾åˆ°ç›¸ä¼¼çš„åŒ»å­¦å½±åƒ

**åº”ç”¨ä»·å€¼**ï¼š
- è¾…åŠ©è¯Šæ–­ï¼ˆæ‰¾åˆ°ç›¸ä¼¼ç—…ä¾‹ï¼‰
- åŒ»å­¦æ•™å­¦ï¼ˆæ ¹æ®æè¿°æ‰¾ç¤ºä¾‹ï¼‰
- ç—…ä¾‹ç ”ç©¶

**æ³¨æ„äº‹é¡¹**ï¼š
- éœ€è¦ä¸“é—¨çš„åŒ»å­¦å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚BiomedCLIPï¼‰
- æ•°æ®éšç§å’Œåˆè§„è¦æ±‚
- éœ€è¦åŒ»ç”Ÿå®¡æ ¸

---

## äº”ã€æŠ€æœ¯å®ç°è¦ç‚¹

### 5.1 å‘é‡æ•°æ®åº“Schemaè®¾è®¡

**å¤šæ¨¡æ€Collectionè®¾è®¡**ï¼š

**æ–¹æ¡ˆ1ï¼šç»Ÿä¸€Collection + æ¨¡æ€æ ‡ç­¾**
```python
schema = {
    "id": "varchar",
    "content_type": "varchar",  # "text", "image", "video", "audio"
    "vector": "float_vector(512)",  # ç»Ÿä¸€ç»´åº¦
    "text_content": "varchar",
    "image_url": "varchar",
    "metadata": "json"
}
```

**æ–¹æ¡ˆ2ï¼šåˆ†ç¦»Collection**
```python
# Text Collection
text_schema = {
    "id": "varchar",
    "vector": "float_vector(1024)",
    "text": "varchar",
    "source_doc_id": "varchar"
}

# Image Collection
image_schema = {
    "id": "varchar",
    "vector": "float_vector(512)",
    "image_url": "varchar",
    "caption": "varchar",
    "source_doc_id": "varchar"
}
```

**æ¨è**ï¼šæ–¹æ¡ˆ2ï¼ˆåˆ†ç¦»ï¼‰ï¼Œç†ç”±ï¼š
- ä¸åŒæ¨¡æ€å¯èƒ½ç”¨ä¸åŒçš„Embeddingæ¨¡å‹ï¼ˆç»´åº¦ä¸åŒï¼‰
- æŸ¥è¯¢æ¨¡å¼ä¸åŒï¼ˆçº¯æ–‡æœ¬ vs çº¯å›¾åƒ vs æ··åˆï¼‰
- ç´¢å¼•ç±»å‹ä¼˜åŒ–ä¸åŒ

### 5.2 è·¨æ¨¡æ€æ£€ç´¢ç­–ç•¥

**åœºæ™¯1ï¼šæ–‡æœ¬æŸ¥è¯¢ â†’ å¤šæ¨¡æ€ç»“æœ**
```python
def text_to_multimodal_search(query_text):
    # æ–‡æœ¬åµŒå…¥
    text_vector = text_embedder.encode(query_text)
    
    # å¹¶è¡Œæ£€ç´¢
    text_results = text_collection.search(text_vector, top_k=10)
    
    # CLIPåµŒå…¥ï¼ˆè·¨æ¨¡æ€ï¼‰
    clip_vector = clip_model.encode_text(query_text)
    image_results = image_collection.search(clip_vector, top_k=5)
    
    # åˆå¹¶ç»“æœ
    return merge_results(text_results, image_results)
```

**åœºæ™¯2ï¼šå›¾åƒæŸ¥è¯¢ â†’ å¤šæ¨¡æ€ç»“æœ**
```python
def image_to_multimodal_search(query_image):
    # å›¾åƒåµŒå…¥
    clip_vector = clip_model.encode_image(query_image)
    
    # æ£€ç´¢ç›¸ä¼¼å›¾åƒ
    image_results = image_collection.search(clip_vector, top_k=10)
    
    # æ£€ç´¢ç›¸å…³æ–‡æœ¬ï¼ˆä½¿ç”¨å›¾åƒå‘é‡ï¼‰
    text_results = text_collection.search(clip_vector, top_k=5)
    
    return merge_results(image_results, text_results)
```

### 5.3 æ€§èƒ½ä¼˜åŒ–

**1. å›¾åƒé¢„å¤„ç†ç¼“å­˜**
```python
# é¿å…é‡å¤ç¼–ç 
@lru_cache(maxsize=1000)
def get_image_embedding(image_path):
    image = load_image(image_path)
    return clip_model.encode(image)
```

**2. æ‰¹é‡ç¼–ç **
```python
# æ‰¹é‡å¤„ç†æå‡æ•ˆç‡
images = load_images_batch(image_paths)
embeddings = clip_model.encode(images, batch_size=32)  # GPUåŠ é€Ÿ
```

**3. é™ç»´ä¸é‡åŒ–**
```python
# åŸå§‹å‘é‡ï¼š512ç»´ float32 â†’ 2KB
# PQå‹ç¼©åï¼š64å­—èŠ‚ â†’ èŠ‚çœ97%ç©ºé—´
from faiss import IndexPQ
index = IndexPQ(512, 64, 8)  # å‹ç¼©åˆ°64å­—èŠ‚
```

---

## å…­ã€å®è·µæ•™ç¨‹ï¼ˆå¾…å¼€å‘ï¼‰

### ğŸ““ Notebook 1: CLIPå›¾æ–‡æ£€ç´¢å…¥é—¨
**æ–‡ä»¶**ï¼š`clip_basics.ipynb`ï¼ˆå¾…å¼€å‘ï¼‰

**å†…å®¹**ï¼š
1. CLIPæ¨¡å‹åŠ è½½ä¸ä½¿ç”¨
2. ä»¥æ–‡æœå›¾å®ç°
3. ä»¥å›¾æœæ–‡å®ç°
4. é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
5. å‘é‡æ•°æ®åº“é›†æˆ

### ğŸ““ Notebook 2: ç”µå•†äº§å“æœç´¢ç³»ç»Ÿ
**æ–‡ä»¶**ï¼š`ecommerce_image_search.ipynb`ï¼ˆå¾…å¼€å‘ï¼‰

**å†…å®¹**ï¼š
1. å•†å“å›¾åƒæ•°æ®é¢„å¤„ç†
2. å¤šæ¨¡æ€ç´¢å¼•æ„å»º
3. ä»¥å›¾æœå•†å“åŠŸèƒ½
4. å¤šæ¡ä»¶è¿‡æ»¤ï¼ˆä»·æ ¼ã€ç±»åˆ«ç­‰ï¼‰
5. ç»“æœæ’åºä¼˜åŒ–

### ğŸ““ Notebook 3: æ™ºèƒ½ç›¸å†Œç®¡ç†
**æ–‡ä»¶**ï¼š`photo_album_search.ipynb`ï¼ˆå¾…å¼€å‘ï¼‰

**å†…å®¹**ï¼š
1. ç…§ç‰‡æ‰¹é‡ç¼–ç 
2. äººè„¸è¯†åˆ«é›†æˆ
3. è‡ªç„¶è¯­è¨€ç…§ç‰‡æœç´¢
4. æ—¶é—´/åœ°ç‚¹è¿‡æ»¤
5. Web UI å®ç°

### ğŸ““ Notebook 4: è§†é¢‘å†…å®¹æ£€ç´¢
**æ–‡ä»¶**ï¼š`video_search.ipynb`ï¼ˆå¾…å¼€å‘ï¼‰

**å†…å®¹**ï¼š
1. è§†é¢‘å…³é”®å¸§æå–
2. å¸§çº§åˆ«ç´¢å¼•æ„å»º
3. åœºæ™¯æ£€ç´¢ä¸å®šä½
4. å­—å¹•è”åˆæ£€ç´¢
5. æ—¶é—´è½´å¯è§†åŒ–

### ğŸ““ Notebook 5: å¤šæ¨¡æ€RAGç³»ç»Ÿ
**æ–‡ä»¶**ï¼š`multimodal_rag_system.ipynb`ï¼ˆå¾…å¼€å‘ï¼‰

**å†…å®¹**ï¼š
1. å¤šæ¨¡æ€æ–‡æ¡£è§£æ
2. ç»Ÿä¸€å‘é‡åº“è®¾è®¡
3. è·¨æ¨¡æ€æ£€ç´¢å®ç°
4. GPT-4Vé›†æˆ
5. å®Œæ•´é—®ç­”æµç¨‹

---

## ä¸ƒã€æ¨¡å‹é€‰æ‹©æŒ‡å—

### 7.1 å¼€æº vs é—­æº

| ç»´åº¦ | å¼€æºæ¨¡å‹ï¼ˆCLIPç­‰ï¼‰ | é—­æºAPIï¼ˆOpenAIç­‰ï¼‰ |
|------|-------------------|---------------------|
| **æˆæœ¬** | å…è´¹ï¼Œéœ€è‡ªéƒ¨ç½² | æŒ‰è°ƒç”¨ä»˜è´¹ |
| **æ€§èƒ½** | ä¸­ç­‰ï¼ŒæŒç»­æ”¹è¿› | æœ€å¥½ |
| **å®šåˆ¶æ€§** | å¯å¾®è°ƒ | æ— æ³•å¾®è°ƒ |
| **éƒ¨ç½²** | éœ€GPU | ç›´æ¥è°ƒç”¨ |
| **æ•°æ®éšç§** | å®Œå…¨å¯æ§ | ä¸Šä¼ åˆ°äº‘ç«¯ |

### 7.2 æ¨¡å‹æ¨è

**å¿«é€ŸåŸå‹ï¼ˆPOCï¼‰**ï¼š
```
æ¨èï¼šOpenAI CLIP API æˆ– Jina AI
ç†ç”±ï¼šå¿«é€ŸéªŒè¯ï¼Œæ— éœ€éƒ¨ç½²
```

**ä¸­æ–‡ä¸ºä¸»**ï¼š
```
æ¨èï¼šChinese CLIP æˆ– Jina CLIP
ç†ç”±ï¼šä¸­æ–‡æ•ˆæœä¼˜åŒ–ï¼Œæ”¯æŒä¸­æ–‡æ–‡æœ¬
```

**ç”Ÿäº§ç¯å¢ƒ**ï¼š
```
æ¨èï¼šè‡ªéƒ¨ç½² CLIP + å‘é‡æ•°æ®åº“
ç†ç”±ï¼šæˆæœ¬å¯æ§ï¼Œæ€§èƒ½ç¨³å®š
```

**è§†é¢‘/éŸ³é¢‘**ï¼š
```
æ¨èï¼šImageBindï¼ˆMetaï¼‰
ç†ç”±ï¼šç»Ÿä¸€å¤šæ¨¡æ€ç©ºé—´ï¼Œæ”¯æŒéŸ³è§†é¢‘
```

---

## å…«ã€å¸¸è§é—®é¢˜

### Q1: CLIPæ¨¡å‹çš„å‘é‡ç»´åº¦å¯ä»¥ä¿®æ”¹å—ï¼Ÿ

**A**: 
```
ä¸å»ºè®®ç›´æ¥ä¿®æ”¹ã€‚CLIPçš„è¾“å‡ºç»´åº¦æ˜¯å›ºå®šçš„ï¼ˆå¦‚512ç»´ï¼‰ã€‚
å¦‚æœéœ€è¦é™ç»´ï¼Œå¯ä»¥ï¼š
1. ä½¿ç”¨PCAé™ç»´
2. è®­ç»ƒä¸€ä¸ªé™ç»´ç½‘ç»œ
3. ä½¿ç”¨PQé‡åŒ–ï¼ˆFaissï¼‰

ä½†é™ç»´ä¼šæŸå¤±ä¸€å®šç²¾åº¦ï¼Œéœ€è¦æƒè¡¡ã€‚
```

### Q2: å¦‚ä½•å¤„ç†ä¸åŒæ¨¡æ€çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼Ÿ

**A**:
```python
# æ–¹æ³•1ï¼šå½’ä¸€åŒ–åè®¡ç®—ï¼ˆæ¨èï¼‰
text_vec_normalized = text_vec / np.linalg.norm(text_vec)
image_vec_normalized = image_vec / np.linalg.norm(image_vec)
similarity = np.dot(text_vec_normalized, image_vec_normalized)

# æ–¹æ³•2ï¼šä½¿ç”¨cosine similarity
from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity([text_vec], [image_vec])[0][0]
```

### Q3: å¤šæ¨¡æ€æ£€ç´¢çš„å»¶è¿Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿ

**A**:
```
1. å›¾åƒé¢„ç¼–ç ç¼“å­˜ï¼ˆç¦»çº¿å¤„ç†ï¼‰
2. æ‰¹é‡ç¼–ç ï¼ˆGPUåŠ é€Ÿï¼‰
3. å‘é‡é‡åŒ–å‹ç¼©ï¼ˆPQ/SQï¼‰
4. åˆ†å±‚æ£€ç´¢ï¼ˆç²—ç­› + ç²¾æ’ï¼‰
5. ä½¿ç”¨æ›´å¿«çš„å‘é‡åº“ï¼ˆMilvus/Qdrantï¼‰
```

### Q4: å¦‚ä½•è¯„ä¼°å¤šæ¨¡æ€æ£€ç´¢æ•ˆæœï¼Ÿ

**A**:
```
æŒ‡æ ‡ï¼š
1. Recall@Kï¼šå‰Kä¸ªç»“æœä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹
2. MRRï¼ˆMean Reciprocal Rankï¼‰ï¼šæ­£ç¡®ç­”æ¡ˆæ’åçš„å€’æ•°å¹³å‡å€¼
3. mAPï¼ˆmean Average Precisionï¼‰ï¼šå¹³å‡ç²¾åº¦å‡å€¼
4. ç”¨æˆ·æ»¡æ„åº¦ï¼šå®é™…ä½¿ç”¨åé¦ˆ

æ–¹æ³•ï¼š
- æ„å»ºæµ‹è¯•é›†ï¼ˆæŸ¥è¯¢-æ­£ç¡®ç»“æœå¯¹ï¼‰
- A/Bæµ‹è¯•ä¸åŒæ¨¡å‹
- äººå·¥è¯„ä¼°å‰10ä¸ªç»“æœçš„ç›¸å…³æ€§
```

---

## ä¹ã€å­¦ä¹ è·¯çº¿

### è·¯çº¿1ï¼šå¿«é€Ÿå…¥é—¨ï¼ˆ1-2å¤©ï¼‰

```
Day 1:
â”œâ”€ ä¸Šåˆï¼šé˜…è¯»æœ¬READMEï¼Œç†è§£å¤šæ¨¡æ€æ¦‚å¿µ
â”œâ”€ ä¸‹åˆï¼šè¿è¡ŒCLIPåŸºç¡€ç¤ºä¾‹
â””â”€ æ™šä¸Šï¼šå®ç°ç®€å•çš„ä»¥å›¾æœå›¾

Day 2:
â”œâ”€ ä¸Šåˆï¼šé›†æˆå‘é‡æ•°æ®åº“
â”œâ”€ ä¸‹åˆï¼šæ„å»ºå°å‹å›¾ç‰‡æœç´¢ç³»ç»Ÿ
â””â”€ æ™šä¸Šï¼šæµ‹è¯•å’Œä¼˜åŒ–
```

### è·¯çº¿2ï¼šæ·±å…¥å®è·µï¼ˆ1å‘¨ï¼‰

```
Week 1:
â”œâ”€ Day 1-2ï¼šCLIPæ¨¡å‹æ·±å…¥å­¦ä¹ 
â”œâ”€ Day 3ï¼šç”µå•†äº§å“æœç´¢å®æˆ˜
â”œâ”€ Day 4ï¼šæ™ºèƒ½ç›¸å†Œå®ç°
â”œâ”€ Day 5ï¼šè§†é¢‘æ£€ç´¢æ¢ç´¢
â”œâ”€ Day 6-7ï¼šå¤šæ¨¡æ€RAGç³»ç»Ÿå¼€å‘
```

### è·¯çº¿3ï¼šé¡¹ç›®åº”ç”¨ï¼ˆæŒç»­ï¼‰

```
1. é€‰æ‹©å®é™…åº”ç”¨åœºæ™¯
2. æ”¶é›†å’Œå‡†å¤‡æ•°æ®
3. é€‰æ‹©åˆé€‚çš„æ¨¡å‹
4. éƒ¨ç½²å‘é‡æ•°æ®åº“
5. å¼€å‘æ£€ç´¢API
6. å‰ç«¯ç•Œé¢å¼€å‘
7. æ€§èƒ½æµ‹è¯•ä¸ä¼˜åŒ–
8. ä¸Šçº¿ä¸è¿­ä»£
```

---

## åã€ç›¸å…³èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [CLIP GitHub](https://github.com/openai/CLIP)
- [ImageBind GitHub](https://github.com/facebookresearch/ImageBind)
- [Chinese CLIP](https://github.com/OFA-Sys/Chinese-CLIP)
- [Jina CLIP](https://jina.ai/embeddings/)

### è®ºæ–‡
- [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020)
- [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/abs/2305.05665)

### åœ¨çº¿Demo
- [CLIP Playground](https://huggingface.co/spaces/openai/clip)
- [ImageBind Demo](https://imagebind.metademolab.com/)

### ç›¸å…³è¯¾ç¨‹
- [Stanford CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
- [Multi-Modal Machine Learning (CMU)](https://cmu-multicomp-lab.github.io/mmml-course/fall2022/)

---

## ä¸‹ä¸€æ­¥

å®Œæˆå¤šæ¨¡æ€åµŒå…¥å­¦ä¹ åï¼Œå¯ä»¥ï¼š

â¡ï¸ è¿”å› [é¡¹ç›®ä¸»é¡µ](../README.md)  
â¡ï¸ æ¢ç´¢ [GraphRAGå›¾æ£€ç´¢](../04_graph_rag/README.md)  
â¡ï¸ å­¦ä¹  [ä¼ä¸šçº§å‘é‡æ•°æ®åº“](../07_vector_database_enterprise/README.md)

---

## è´¡çŒ®æŒ‡å—

æœ¬ç« èŠ‚æ­£åœ¨æŒç»­å®Œå–„ä¸­ï¼Œæ¬¢è¿è´¡çŒ®ï¼š

**å¾…å¼€å‘å†…å®¹**ï¼š
- [ ] CLIPåŸºç¡€æ•™ç¨‹ï¼ˆclip_basics.ipynbï¼‰
- [ ] ç”µå•†äº§å“æœç´¢ï¼ˆecommerce_image_search.ipynbï¼‰
- [ ] æ™ºèƒ½ç›¸å†Œç®¡ç†ï¼ˆphoto_album_search.ipynbï¼‰
- [ ] è§†é¢‘å†…å®¹æ£€ç´¢ï¼ˆvideo_search.ipynbï¼‰
- [ ] å¤šæ¨¡æ€RAGç³»ç»Ÿï¼ˆmultimodal_rag_system.ipynbï¼‰
- [ ] ImageBindå¤šæ¨¡æ€åº”ç”¨
- [ ] éŸ³é¢‘æ£€ç´¢å®æˆ˜
- [ ] å¤šæ¨¡æ€æ¨¡å‹å¾®è°ƒ

**å¦‚ä½•è´¡çŒ®**ï¼š
1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. æäº¤ä»£ç å’Œæ–‡æ¡£
4. å‘èµ· Pull Request

---

ğŸ’¡ **å¯„è¯­**ï¼šå¤šæ¨¡æ€æ˜¯AIçš„æœªæ¥æ–¹å‘ï¼æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘çš„èåˆå°†å¸¦æ¥æ›´ä¸°å¯Œçš„åº”ç”¨åœºæ™¯ã€‚è™½ç„¶æŠ€æœ¯è¿˜åœ¨å¿«é€Ÿå‘å±•ï¼Œä½†ç°åœ¨å°±æ˜¯æœ€å¥½çš„å­¦ä¹ æ—¶æœºã€‚

**Let's explore the multimodal world! ğŸš€ğŸ¨ğŸµğŸ¬**

